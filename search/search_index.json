{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to the DepledgeLabWiki!","text":"<p>Work in progress</p> <p>THIS WIKI IS CURRENTLY UNDER CONSTRUCTION AND CONTAINS MANY PLACEHOLDERS. THE MONDAY LAB MEETING SCHEDULE IS BEING REGULARY UPDATED THOUGH.</p> <p>For a full view of currently available pages check out the tag directory. Thank you for your patience. ~Erik</p>"},{"location":"#the-purpose-of-this-wiki","title":"The purpose of this Wiki","text":"<p>This wiki is the central documentation hub of the DepledgeLab at the Hannover Medical School (MHH), Institute of Virology.</p> <p>It means to be the home of important protocols, scripts, bioinformatical methods, guides and much more! DepledgeLabWiki aims to - over time - create a reference point for present and future researchers of AG Depledge to write down, share, explain and retain acquired knowledge. It is also written with the intent to inform collaborators about the details of AG Depledges work. </p>"},{"location":"#who-manages-this-wiki-and-how-do-i-contribute","title":"Who manages this Wiki and how do I contribute?","text":"<p>At the moment, Erik manages the Wiki and most entries are written by Dan or Erik. If you have a field of expertise, an interesting pipeline or something else, you are free and encouraged to add your knowledge to this Wiki! Please contact Dan/Erik for further information. To find out more about editing this Wiki, check out Editing this Wiki.  If you have any questions, comments, suggestions or else regarding this Wiki, please contact Erik. </p>"},{"location":"#where-do-i-find-what","title":"Where do I find what?","text":"<p>This Wiki is still very much under construction and a lot is still missing. To get an overview of currently available pages, check out the tag directory. </p>"},{"location":"tags/","title":"Tags","text":"<p>Here you can find the comprehensive list of all tags that are being used across the Wiki. This site also serves as n overview of all currently available entries.</p>"},{"location":"tags/#tag:alignment","title":"Alignment","text":"<ul> <li>            Fun with (alignment) Flags          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:basecalling","title":"Basecalling","text":"<ul> <li>            Basecalling          </li> </ul>"},{"location":"tags/#tag:bash","title":"Bash","text":"<ul> <li>            Basecalling          </li> <li>            E.C.S.T.A.S.Y.          </li> <li>            Fun with (alignment) Flags          </li> <li>            Metaplots          </li> <li>            Useful Commands          </li> <li>            WarpDemuX          </li> </ul>"},{"location":"tags/#tag:cell-culture","title":"Cell Culture","text":"<ul> <li>            Disposal of Cell culture waste          </li> <li>            Intro to Cell culture          </li> <li>            Useful numbers for Cell culture          </li> </ul>"},{"location":"tags/#tag:drs","title":"DRS","text":"<ul> <li>            Basecalling          </li> <li>            Ninetails          </li> <li>            WarpDemuX          </li> </ul>"},{"location":"tags/#tag:dates","title":"Dates","text":"<ul> <li>            Monday Lab meetings          </li> </ul>"},{"location":"tags/#tag:dorado","title":"Dorado","text":"<ul> <li>            Basecalling          </li> </ul>"},{"location":"tags/#tag:flags","title":"Flags","text":"<ul> <li>            Fun with (alignment) Flags          </li> </ul>"},{"location":"tags/#tag:guide","title":"Guide","text":"<ul> <li>            Editing this Wiki          </li> <li>            Further Information on this Wiki          </li> <li>            Useful Commands          </li> </ul>"},{"location":"tags/#tag:instruction","title":"Instruction","text":"<ul> <li>            Disposal of Cell culture waste          </li> <li>            Emergency procedure for phenol/trizol          </li> <li>            Wet-lab 101          </li> </ul>"},{"location":"tags/#tag:loremipsum","title":"LoremIpsum","text":"<ul> <li>            Topic 1          </li> </ul>"},{"location":"tags/#tag:metaplot","title":"Metaplot","text":"<ul> <li>            Metaplots          </li> </ul>"},{"location":"tags/#tag:miscellaneous","title":"Miscellaneous","text":"<ul> <li>            E.C.S.T.A.S.Y.          </li> <li>            Overview          </li> <li>            Overview          </li> </ul>"},{"location":"tags/#tag:nanopore","title":"Nanopore","text":"<ul> <li>            Basecalling          </li> <li>            Ninetails          </li> <li>            WarpDemuX          </li> </ul>"},{"location":"tags/#tag:ninetails","title":"Ninetails","text":"<ul> <li>            Ninetails          </li> </ul>"},{"location":"tags/#tag:protocol","title":"Protocol","text":"<ul> <li>            Intro to Cell culture          </li> <li>            Useful numbers for Cell culture          </li> </ul>"},{"location":"tags/#tag:python","title":"Python","text":"<ul> <li>            E.C.S.T.A.S.Y.          </li> </ul>"},{"location":"tags/#tag:r","title":"R","text":"<ul> <li>            E.C.S.T.A.S.Y.          </li> <li>            Metaplots          </li> <li>            Ninetails          </li> </ul>"},{"location":"tags/#tag:warpdemux","title":"WarpDemuX","text":"<ul> <li>            WarpDemuX          </li> </ul>"},{"location":"tags/#tag:website","title":"Website","text":"<ul> <li>            Link collection (Website)          </li> </ul>"},{"location":"tags/#tag:wiki","title":"Wiki","text":"<ul> <li>            Editing this Wiki          </li> <li>            Further Information on this Wiki          </li> </ul>"},{"location":"tags/#tag:alignment","title":"alignment","text":"<ul> <li>            Pseudoalignment with Kallisto          </li> </ul>"},{"location":"tags/#tag:kallisto","title":"kallisto","text":"<ul> <li>            Pseudoalignment with Kallisto          </li> </ul>"},{"location":"tags/#tag:pseudoalignment","title":"pseudoalignment","text":"<ul> <li>            Pseudoalignment with Kallisto          </li> </ul>"},{"location":"Getting_started/general_info/","title":"Overview","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Getting_started/Student_help/phd_tips/","title":"Tips for PhD Students","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Getting_started/Student_help/theses_tips/","title":"Tips for Theses","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Getting_started/basic_drylab/intro_drylab/","title":"Page 1","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Getting_started/basic_drylab/Installation_help/installing_R/","title":"R(Studio)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Getting_started/basic_drylab/Installation_help/installing_citrix/","title":"MHH-VPN (Citrix)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Getting_started/basic_drylab/Installation_help/installing_hpcaccess/","title":"Get HPC access","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Getting_started/basic_wetlab/intro_wetlab/","title":"Page 1","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Home/news/","title":"News","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Home/How_to_use_this_Wiki/read_search/","title":"Manoeuver through the Wiki","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/","title":"Editing this Wiki","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#introduction","title":"Introduction","text":"<p>Hello there, fellow researcher! This page is written for all current and future members of AG Depledge and acts as a guiding rope in how to edit this wiki. As the long-term goal of this Wiki is to become an extensive documentation hub, a uniform coding and organising style is essential. Because of this, please read through this page before making any changes and make yourself familiar with the mentioned topics. Throughout this page, different text formatting has been used deliberately. That way, this page also serves as a reference if you want to include more sophisticated formatting elements. However, don't be alarmed by the length of this page or some sections that look complicated, most of this page is optional.</p> <p>Should you have any questions, feel free to ask Erik for more information. <sub> (That is, if I'm still around by the time you are reading this.) </sub></p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#general-information","title":"General Information","text":"<p>This Wiki is based on the MkDocs framework, which is a common static site generator built for project documentation. To be more precise, it uses an extension for MkDocs, Materials for MkDocs.</p> Where Materials for MkDocs comes from <p>Sometime Materials for MkDocs is also referred to as a \"theme\" of MkDocs. MkDocs allows so called \"themes\" to change its appearance (and behavior). It contains only few themes from the get-go, but there are many independent ones. Materials for MkDocs started out as such a third party theme and since evolved into a full framework itself. The installation of MkDocs is still necessary though.</p> <p>This Wiki (which is essentially just the collection of all its files and folders) is hosted on GitHub and deployed through GitHub Pages (using GitHubs automatic Action workflows)(1). The actual repository link of this Wiki is therefore \"https://github.com/DepledgeLab/DepledgeLabWiki\" (name might change). From here, it hosts the actual Wiki site you can currently see, \"https://depledgelab.github.io/DepledgeLabWiki/\".</p> <ol> <li>...which in turn, is based on Jekyll, which in turn is written in Ruby.</li> </ol>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#where-to-find-out-more","title":"Where to find out more","text":"<p>Recommendation</p> <p>The documentation for Materials for MkDocs is very extensive and well written and also the most important reference for this Wiki. It is highly recommended to give it a visit.</p> Documentation resource Link MkDocs Click me Materials for MkDocs \u2192 Click me \u2190 GitHub Pages Click me Markdown Click me HTML Click me CSS Click me Git Click me","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#working-with-the-repository","title":"Working with the repository","text":"<p>The actual content of this Wiki lies at \"https://github.com/DepledgeLab/DepledgeLabWiki\". From the files available there, all sites are rendered to this Wiki. This means that if you want to change, add or remove any content you will have to edit, add, or remove the appropriate files in the mentioned GitHub repository. In most cases you will only want to change one of these files:</p> <code>The .md (markdown) files</code> The markdown files are the files that contain the actual content of the sites. One markdown file equals to one site on the Wiki. For example, you can find the content of this page you are currently reading at \"docs/Home/How_to_use_this_Wiki/Edit/editing_this_wiki.md\" (in the GitHub repository)(name might change). If you were to change anything in that file the changes would reflect in the rendered Wiki. <code>The mkdocs.yml file</code> This is the main configuration file of this Wiki and could be considered the heart. It contains information on the structure of this Wiki, which themes and plugins are enabled etc. More info on that file can be found at Further Information on this Wiki. <p>This leads us to the most important question in this guide:</p> <p>How do I actually edit the Wiki?</p> <p>There are many different options. Because of this you can find a collection of possible solutions below. You are free to use whatever method suits you best, just keep the listed (dis-)advantages in mind.  If you already have some coding experiences, Erik strongly recommends the Local - Git + IDE (VSCode) solution.  Otherwise, it is recommended to stick to solution 1. It has a slimmer selection of features (i.e. you can't look at the changes you have made live), but it's less convoluted. If you choose this option, just ignore the other.</p> <p></p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#first-steps","title":"First steps","text":"<p>Either way, you will have to do the following steps to be able to edit the Wiki:</p> <ol> <li>Get yourself a GitHub Account (if you do not already have one).</li> <li>Familiarise yourself with the basics of GitHub. (If you are opting to use a more extensive solution (Local repo), you should also learn the basics of git. Failure to do so can easily break the Wiki.)</li> <li>Ask Dan for access to the Wiki repository.(1)</li> </ol> <ol> <li>Note to Dan: In the repositories settings just click on Collaborators (left) and add people.</li> </ol> <p></p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#option-1-remote-standard-github-editor-through-github-or-the-wiki-itself","title":"Option 1: Remote - Standard GitHub editor (through GitHub or the Wiki itself)","text":"pros cons  Super quick and easy changes  No live preview  No local installations needed  Messy history  No advanced knowledge needed  Limited editing (e.g. no deletion of files)  Longer reloading times (Wiki reboots with every save)  Many changes in quick succession might hit API limits and restrict changes temporarily <ol> <li>Login into your GitHub account in your web browser.</li> <li>Navigate to the file you want to edit on GitHub and click on Edit this file on the right.ORNavigate to the site you want to edit in the Wiki and click on Edit this page in the top right.</li> <li>Edit and save the file.</li> </ol>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#option-2-remote-githubs-integrated-vscode","title":"Option 2: Remote - GitHubs integrated VSCode","text":"pros cons  Quick changes  No live preview  No local installations needed <ol> <li>Login into your GitHub account in your web browser.</li> <li>Navigate to the file you want to edit on GitHub and press . (on your keyboard) to open the integrated VSCode editor.</li> <li>Edit and save your files (Tip: Local files can be added with drag-and-drop).</li> <li>Commit and push them (left sidebar \u2192 Source Control).</li> </ol>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#option-3-local-git-only","title":"Option 3: Local - Git only","text":"pros cons  Allows changes (and saves) to a local instance before remote pushes  Requires (slightly longer) initial setup  Cleaner history  No live preview  No IDE installation necessary  Minor Git and Linux knowledge necessary Initial (one-time) setupThe work routine afterwards <p>If you have already done parts of the following steps you can just skip them.</p> <ol> <li>Install Git on your local machine (Download; changing settings not necessary).</li> <li>Create a local repository (this will be your local instance of the Wiki).<ol> <li>Open the Git bash and navigate to a folder you want to be your local repository to be in.</li> <li>Initiate the repository and add the Wiki repository as its remote origin (upstream).</li> <li>(Recommended: rename your branch to main instead of master, as this is the name the Wiki repository uses.)</li> </ol> </li> <li>(Check if you already have a ssh key.)<ol> <li>Type <code>cd ~/.ssh</code> and <code>ls</code>.</li> <li>Check for a file named \"id_dsa(.pub)\" or \"id_rsa(.pub)\". If these files do not exist, or the command above does not work, you do not have a ssh key yet.</li> </ol> </li> <li>Create a personal ssh-key if you do not already have one (this is necessary to authorize your local machine to GitHub).<ol> <li>In the Git bash, type <code>ssh-keygen</code>.</li> <li>Press Enter thrice.</li> <li>Go to ~/.ssh (yourUser/.ssh) and copy the public key (content of the .pub file).</li> </ol> </li> <li>Add your ssh key to GitHub.<ol> <li>Open the web browser of your choice and login to GitHub.</li> <li>Go to your profile settings.</li> <li>Go to Access \u2192 SSH and GPG keys.</li> <li>Click on new SSH key.</li> <li>Paste and add your public ssh key you just copied.</li> </ol> </li> <li>Synchronise with the remote repository.<ol> <li>In the Git bash, set your username and email (corresponding to your GitHub account) (<code>git config user.name XXX</code>, <code>git config user.email XXX</code>).</li> <li>Go to your local repository in the Git bash and pull the Wiki (<code>git pull origin main</code>).</li> </ol> </li> </ol> <ol> <li>Pull from remote if necessary.</li> <li>Locally edit files (with an editor of your choice).</li> <li>Commit changes and push them to remote. Merge if necessary.</li> </ol>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#option-4-local-git-ide-vscode-recommended","title":"Option 4: Local - Git + IDE (VSCode) [Recommended]","text":"pros cons  Once set up very quick to use  Requires (slightly longer) initial setup  Allows changes (and saves) to a local instance before remote pushes  Small changes in YAML file necessary whenever pull/push (difference in configuration for remote/local)  Allows (almost) instant local live previews  Minor Git and Linux knowledge necessary  Cleaner history  Good IDE with many interfaces Initial (one-time) setupThe work routine afterwards <p>This guide will use VSCode as its IDE. If you have already done parts of the following steps you can just skip them.</p> <ol> <li>Install Git on your local machine (Download; changing settings not necessary).</li> <li>Create a local repository (this will be your local instance of the Wiki).<ol> <li>Open the Git bash and navigate to a folder you want to be your local repository to be in. (Optional: You might want to create a subfolder for your local repository, that way you can add your python environment without it interfering with git (see directory structure below).)</li> <li>Initiate the repository and add the Wiki repository as its remote origin.</li> <li>(Recommended: rename your branch to main instead of master, as this is the name the Wiki repository uses.)</li> </ol> </li> <li>Install Python on your local machine (Download).</li> <li>Install VSCode on your local machine (Download).<ol> <li>Add extensions (Python, GitHub Pull Requests) (and many more that are useful (optional)).</li> <li>In VScode, go to the parent folder of your local repository (built-in terminal) and create a new python environment (Ctrl+Shift+P \u2192 \"Create Environment\" \u2192 Choose a python version). (You can also use conda, but this example will use a standard python environment.)</li> <li>Open the python terminal and install necessary packages: <pre><code>pip install mkdocs-material\npip install mkdocs-git-revision-date-localized-plugin\npip install mkdocs-git-committers-plugin-2\n</code></pre></li> <li>Login to your GitHub account on VSCode (left sidebar)</li> </ol> </li> <li>Setup<ol> <li>Pull from remote (left sidebar \u2192 Source control \u2192 Pull)</li> </ol> </li> </ol> <p>Unless you used conda or created your python environment differently, your files should now be arranged like this (including hidden files): <pre><code>.\n\u251c\u2500 .venv\n\u2502  \u2514\u2500 ...\n\u2514\u2500 wiki_local\n   \u251c\u2500 .cache\n   \u251c\u2500 .github\n   \u251c\u2500 docs\n   \u251c\u2500 includes\n   \u251c\u2500 site\n   \u251c\u2500 LICENSE\n   \u251c\u2500 mkdocs.yml\n   \u251c\u2500 README.md\n   \u2514\u2500 test.py\n</code></pre></p> <ol> <li>Pull from remote if necessary.</li> <li>After pulling from remote:<ol> <li>Turn off the committers and revision plugins. (These plugin add the creation and last-updated time at the bottom of a site. Using this locally technically still works but increases the live preview time massively.)<ol> <li>Open the <code>mkdocs.yml</code> file and scroll to <code># GitHub: commented out; local: not commented out</code>.</li> <li>Delete the <code>#</code> in front of <code>enabled:</code>. These two lines should be active locally. Don't forget to save the changes (Ctrl+S).</li> </ol> </li> <li>Change the <code>site_url</code> to local<ol> <li>Open the <code>mkdocs.yml</code> file and scroll to the very top</li> <li>Un-comment (remove the <code>#</code> in front) <code>#site_url: http://127.0.0.1:8000/</code> and comment (add a <code>#</code> in front) <code>site_url: https://depledgelab.github.io/DepledgeLabWiki/</code></li> </ol> </li> </ol> </li> <li>In the VSCode Terminal: type <code>mkdocs serve</code> (make sure you are in the folder that contains the Wiki).<ol> <li>The git-committers plugin should be DISABLED (Ctrl+C to abort).</li> <li>Paste the local address into your web browser.</li> <li>Edits you do are now reflected live (Ctrl+S to save a file and update the Wiki).</li> <li>Use Ctrl+C in the VSCode terminal to shut down.</li> </ol> </li> <li>Edit files locally (via VSCode).</li> <li>Before pushing to the remote again:<ol> <li>Turn on the committers and revision plugins.<ol> <li>Open the <code>mkdocs.yml</code> file and scroll to <code># GitHub: commented out; local: not commented out</code>.</li> <li>Add a <code>#</code> in front of <code>enabled:</code>. These two lines should be deactivated in the remote repository. Don't forget to save the changes.</li> </ol> </li> <li>Change the <code>site_url</code> to remote<ol> <li>Open the <code>mkdocs.yml</code> file and scroll to the very top</li> <li>Un-comment <code>#site_url: https://depledgelab.github.io/DepledgeLabWiki/</code> and comment <code>site_url: http://127.0.0.1:8000/</code></li> </ol> </li> </ol> </li> <li>Commit changes and push them to remote. Merge if necessary.</li> </ol> <p>Note</p> <p>Bigger changes to <code>mkdocs.yml</code> sometimes require to restart the Wiki (Ctrl+C + <code>mkdocs serve</code>).</p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#adding-a-new-page","title":"Adding a new page","text":"","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#obligatory-changes","title":"Obligatory changes","text":"<p>In order to add a new page to the Wiki, you have to do just two things:</p> <ol> <li>Create a new .md file in the docs folder. Create subfolders if necessary. Best practise is, to mirror the layout of the wiki to the folder strucutre (e.g.: The monday lab meeting schedule is located at Home -&gt; Schedules and Events -&gt; Monday Lab meetings. Conversely, the corresponding .md file is located at /docs/Home/Schedules_and_Events/monday_meeting.md.)(1)</li> <li>Determine where and with which name the new page should be visible in the Wiki. For this, open the mkdocs.yml file. Under '### Structure of the Wiki', the layout of the Wiki is specified. Each bullet point is one entry in the Wiki.(2) Here, you enter your new page with the format 'Name of the page: pathtothefile/file.md'</li> </ol> <ol> <li>The way the Wiki is currently set up, they don't have to match, as the Wiki itself doesn't really care where the files are; but it's more organised this way.</li> <li>Bullet points with no .md files attached to them are chapters (see the content table on the left), they don't have their own page. Chapters must contain at least one page underneath them.</li> </ol> <p>Example:  Say this is how the docs folder looks like (a section of it):</p> <pre><code>docs\n\u251c\u2500 Getting_started\n\u2502  \u2514\u2500 ...\n\u251c\u2500 Home\n\u2502   \u251c\u2500 How_to_use_this_Wiki\n\u2502   \u2502   \u251c\u2500 Edit\n\u2502   \u2502   \u2502   \u251c\u2500 editing_this_wiki.md\n\u2502   \u2502   \u2502   \u2514\u2500 further_wiki_info.md\n\u2502   \u2502   \u2514\u2500 read_search.md\n\u2502   \u251c\u2500 Schedules_and_Events\n\u2502   \u2502  \u2514\u2500 ...\n\u2502   \u2514\u2500 news.md\n\u251c\u2500 How-to_Dry-lab\n\u2502   \u2514\u2500 ...\n...\n</code></pre> <p>And we wanted to add a new chapter under 'How_to_use_this_Wiki' called 'Changing the layout' and add a page to that called 'Why darkmode is superior'. We would first create the needed folders and .md file, so our new structure looks like this:</p> <pre><code>docs\n\u251c\u2500 Getting_started\n\u2502  \u2514\u2500 ...\n\u251c\u2500 Home\n\u2502   \u251c\u2500 How_to_use_this_Wiki\n\u2502   \u2502   \u251c\u2500 Edit\n\u2502   \u2502   \u2502   \u251c\u2500 editing_this_wiki.md\n\u2502   \u2502   \u2502   \u2514\u2500 further_wiki_info.md\n\u2502   \u2502   \u251c\u2500 read_search.md\n\u2502   \u2502   \u2514\u2500 Change_layout\n\u2502   \u2502       \u2514\u2500 darkmode.md\n\u2502   \u251c\u2500 Schedules_and_Events\n\u2502   \u2502  \u2514\u2500 ...\n\u2502   \u2514\u2500 news.md\n\u251c\u2500 How-to_Dry-lab\n\u2502   \u2514\u2500 ...\n...\n</code></pre> <p>Once we have written something into the file (and saved it) we add it to our table in the mkdocs.yml file.</p> <p>Previously, the respective session may have looked something like this:</p> <pre><code>- Home:\n    - Welcome: index.md\n    - News: Home/news.md\n    - Schedules and Events:\n      - ...\n    - How to use this Wiki:\n      - Read, Search and Tags:\n        - Manoeuver through the Wiki: Home/How_to_use_this_Wiki/read_search.md\n        - List of Tags: tags.md\n      - Manage:\n        - Editing this Wiki: Home/How_to_use_this_Wiki/Edit/editing_this_wiki.md\n        - Further Information on this Wiki: Home/How_to_use_this_Wiki/Edit/further_wiki_info.md\n  - Getting Started:\n    - ...\n...\n</code></pre> <p>After our edit, it might look like this:</p> <pre><code>- Home:\n    - Welcome: index.md\n    - News: Home/news.md\n    - Schedules and Events:\n      - ...\n    - How to use this Wiki:\n      - Read, Search and Tags:\n        - Manoeuver through the Wiki: Home/How_to_use_this_Wiki/read_search.md\n        - List of Tags: tags.md\n      - Manage:\n        - Editing this Wiki: Home/How_to_use_this_Wiki/Edit/editing_this_wiki.md\n        - Further Information on this Wiki: Home/How_to_use_this_Wiki/Edit/further_wiki_info.md\n      - Changing the layout:\n        - Why darkmode is superior: Home/How_to_use_this_Wiki/Change_layout/darkmode.md\n  - Getting Started:\n    - ...\n...\n</code></pre> <p>Once both has been done, saved and pushed to the repository, it should be updated after a few minutes.</p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#additional-optional-things-to-add-for-better-qol","title":"Additional (optional) things to add (for better QoL):","text":"<p>You can also add tags to a page (see at top at this page). These help categorising the Wiki. The Wiki automatically creates an overview of all pages associated with a certain tag at the tags page. Adding tags is technically not obligatory for the Wiki to work, but makes it significantly easier to find something. How to add a tag</p> <p>If your text contains common abbreviations, you can also add them to the Wiki-wide abbreviation index. That way,  whenever that abbreviation is used anywhere in the Wiki, it's explanation is also automatically rendered (example: Using your mouse, hover over this abbreviation: QoL). How to add abbreviations</p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#writing-content","title":"Writing content","text":"","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#plain-text","title":"Plain text","text":"<p>The content of this Wiki is written in Markdown. This means that plain text can be written as is and be lightly modified via the Markdown syntax (Check this link to see a table of options). For example, this includes bold text, italic text or block quotes:</p> <p>In the pursuit of great, we failed to do good.</p> <p>For more nuanced ways to format your text, you can use HTML. If even more specific modifications are required (e.g.  colored  text), you can use CSS.</p> Endless possibilities <p>Theoretically, you could go really crazy with this if you wanted to. That doesn't mean you have to, or even that you should (in fact, keeping it simple is often a good approach). But it is possible.</p> <p>Info</p> <p>Materials for MkDcos also offers various markdown-like inline text modifications (underlining, highlighting, <sub>sub-</sub> and <sup>superscripts</sup>, etc.) that can be very useful.</p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/editing_this_wiki/#everything-beyond-plain-text","title":"Everything beyond plain text","text":"<p>For an extensive list of all additional annotation features (admonitions, content tabs, tooltips etc.) visit the Materials for MkDcos Reference page. Especially the admonitions are very nice!</p> <p>Info</p> <p>I am an admonition!</p> <p>Because Materials for MkDocs already offers many formatting options it is recommended to check its documentation before trying to implement something manually with HTML or CSS.</p> <p>This also includes more complex structures including these (but not limited to):</p> Code block exampleDiagram exampleFormula example <p>An exemplary code block made with Materials for MkDocs including line numbers, line highlighting and correct syntax highlighting: Dummy_script.R<pre><code>for (i in (1:length(mRNA_IDs))) {\nk &lt;- 1\nfor (j in (1:nrow(exon_IDs))) {\n    if (str_detect(exon_parents[j], mRNA_IDs[i])) {\n    df_all_mRNA_w_exons[k, i, 1] &lt;- exon_IDs[j, 1]\n    df_all_mRNA_w_exons[k, i, 2] &lt;- exon_IDs[j, 2]\n    k &lt;- k + 1\n    }\n  }\n}\n</code></pre></p> <p>An exemplary mermaid diagram made with Materials for MkDocs: <pre><code>graph LR\nA[Hungry] --&gt; B{Eat chocolate?};\nB --&gt;|No| C[Disappointment];\nC --&gt; D{Are you happy?};\nD --&gt;|No| B;\nB ----&gt;|Yes| E[Yummy!];</code></pre></p> <p>An exemplary Mathjax formula</p> \\[ \\cos x=\\sum_{k=0}^{\\infty}\\frac{(-1)^k}{(2k)!}x^{2k} \\]","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/","title":"Further Information of this Wiki","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#further-information","title":"Further information","text":"<p>TODO: more detailed</p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#files-systems-naming-convention","title":"Files systems / naming convention","text":"<ul> <li>Important info on navigation.</li> <li>Structure not determined by directories, but by <code>nav:</code> in <code>mkdocs.yml</code>. Paths still need to be correct.</li> <li>Current attempt: same naming scheme/directory structure between filenames/pagenames/pageheaders for better overview. (But wouldn't be obligatory.)</li> <li>Site URLs are generated from directory and file names.</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#the-mkdocsyml-file","title":"The mkdocs.yml file","text":"<ul> <li>Main configuration file</li> <li>Includes general configurations, Wiki structure, cosmetics (e.g. themes and colors), extensions and plugins.</li> <li>Only edit the structure/ tags, unless you know what you are doing</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#the-githubworkflowsciyml-file","title":"The .github/workflows/ci.yml file","text":"<ul> <li>gives settings for the GitHub Actions deployement of the site.</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#the-cache-folder","title":"The .cache folder","text":"<ul> <li>contains cached data, mainly (only?) git committers data. Greatly reduces load time.</li> <li><code>.cache</code> is listed in <code>.gitignore</code> to prevent merge unnecessary conflicts.</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#the-stylesheetsextracss-file","title":"The stylesheets/extra.css file","text":"<ul> <li>contains additional text formatting settings.</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#adding-tags","title":"Adding tags","text":"<ul> <li>Add the new tag name and reference name in the YAML (mkdocs.yml) file (bottom part, at '## Add new tags here').</li> <li>(Optional but recommended) Give the tag an icon (using the reference name) in the YAML file (top part, at '## Add tag icons here').</li> <li>Add the tag name to the head of an .md file. (Example below)</li> <li>The Wiki currently lists all tags at tags.md (automatically).</li> <li>The .md page that hosts the tag list can be found at docs/tags.md</li> </ul> <p>Example: Head of this .md file<pre><code>---\ntags:\n    - Wiki\n    - Guide\n---\n</code></pre></p>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#adding-abbreviations-the-includesabbreviationsmd-file","title":"Adding abbreviations (the includes/abbreviations.md file)","text":"<ul> <li>Full text automatically pops up as a tooltip (e.g.: hover over the term \"HTML\" on any page).</li> <li>just add an abbreviation in includes/abbreviations.md in the format <code>*[TIAE]: This Is An Example</code>.</li> <li>The rest is done by the configurations, tooltips work globally.</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#currently-activated-plugins","title":"Currently activated plugins","text":"<ul> <li>see <code>mkdocs.yml</code></li> <li>check the Materials for MkDocs documentation for more information.</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/How_to_use_this_Wiki/Edit/further_wiki_info/#the-site-folder-and-offline-builds","title":"The site folder and offline builds","text":"<ul> <li>use <code>mdocs build</code> to manually generate a static build (\u2192 site). (expert only)</li> </ul>","tags":["Wiki","Guide"]},{"location":"Home/Schedules_and_Events/monday_meeting/","title":"Monday Lab Meetings","text":"<p>Work in progress</p> <p>This Wiki (and the webpage) are currently under construction, many parts still have an unfinished look / just contain filler content. The Monday Lab Meeting schedule is correct though. For a full view of currently available pages check out the tag directory. ~Erik</p>","tags":["Dates"]},{"location":"Home/Schedules_and_Events/monday_meeting/#time-table","title":"Time table","text":"<p>Last updated: See the bottom of this page.  </p> NEW: 05.01.26-02.03.2613.10.25-29.12.2521.07.25-06.10.2519.05.25-14.07.2517.03.25-12.05.2520.01.25-10.03.25 Date Time Type of meeting Presenter/Topic/Paper Comment Monday,January\u00a005 CANCELLED Monday,January\u00a012 CANCELLED Monday,January\u00a019 14:30 <code>General</code> Tuesday,January\u00a020 12:30 Viro Journal Club Pierina Monday,January\u00a026 14:30 <code>Journal Discussion</code> mRNA poly(A)-tail length is a battleground for coronavirus-host competition Moderator: Erik Monday,February\u00a002 14:30 <code>Ask Me Anything</code> Aligners:When to use what?Different aligners, references and parameters Tuesday,February\u00a019 12:30 Viro Journal Club Arsa Monday,February\u00a009 CANCELLED Dan away Monday,February\u00a016 CANCELLED Dan away Monday,February\u00a023 14:30 <code>Individual Presenter</code> Erik Monday,March\u00a002 14:30 <code>Individual Presenter</code> Pierina Date Time Type of meeting Presenter/Topic/Paper Comment Monday,October\u00a013 CANCELLED Monday,October\u00a020 CANCELLED Monday,October\u00a027 14:30 <code>Individual Presenter</code> Marius Monday,November\u00a003 14:30 <code>Individual Presenter</code> <code>Journal Discussion</code> MaxtRNA modifications tune m<sup>6</sup>A-dependent mRNA decay Moderator: Elene Monday,November\u00a010 14:30 <code>Journal Discussion</code> <code>Individual Presenter</code> tRNA modifications tune m<sup>6</sup>A-dependent mRNA decayMax Moderator: Elene Wednesday,November\u00a012 13:00 Viro Seminar Erik Monday,November\u00a017 14:30 <code>Ask Me Anything</code> MobaXTerm/HPC/Terminal:Good practices &amp; tips Monday,November\u00a024 14:30 <code>General</code> <code>Individual Presenter</code> Arsa Friday,November\u00a028 09:00 Viro Club Carolin Monday,December\u00a001 14:30 <code>Special</code> Guest talk/discussion:Yannick\u00a0Frommeyer(tRNA) Monday,December\u00a008 14:30 <code>Journal Discussion</code> Immune evasion through mitochondrial transfer in the tumour microenvironmentPseudouridine prevalence in Kaposi\u2019s sarcoma\u2013associated herpesvirus transcriptome reveals an essential mechanism for viral replication Moderator: Ruchira Friday,December\u00a012 09:00 Viro Club Pierina Monday,December\u00a015 14:30 <code>Special</code> Christmas market! :) Wednesday,December\u00a017 13:00 Viro Seminar Ruth Monday,December\u00a022 CANCELLED Monday,December\u00a029 CANCELLED Date Time Type of meeting Presenter/Topic/Paper Comment Monday,July\u00a021 CANCELLED Monday,July\u00a028 CANCELLED Monday,August\u00a004 CANCELLED Monday,August\u00a011 CANCELLED Monday,August\u00a018 14:30 <code>Individual Presenter</code> Sae + Sae's farewell Tuesday,August\u00a019 12:30 Viro Journal Club Joana Monday,August\u00a025 14:30 <code>Ask Me Anything</code> Comparison of Sequencing platforms:Illumina, Nanopore &amp; PacBio + Niclas' farewell Monday,September\u00a001 14:30 <code>General</code> Tuesday,September\u00a003 13:00 Viro Seminar Joana Monday,September\u00a008 14:30 <code>Journal Discussion</code> High-density resolution of the Kaposi's sarcoma associated herpesvirus transcriptome identifies novel transcript isoforms generated by long-range transcription and alternative splicing Moderator: Hanan Wednesday,September\u00a010 13:00 Viro Seminar Pierina Friday,September\u00a012 09:00 Viro Club Erik  Monday,September\u00a015  14:30 <code>Ask Me Anything</code> <code>Individual Presenter</code> CHIPseq, RNAseq, SLAMseq, XXXseq...All the seqs: Which do we use and when? Which seqs are out there?State of the Union Dan &amp; Nora  Monday,September\u00a022  14:30 <code>Journal Discussion</code> <code>Individual Presenter</code> tRNA modifications tune m<sup>6</sup>A-dependent mRNA decayHuda Moderator: EleneHuda's farewell  Monday,September\u00a029  14:30 <code>General</code> <code>Journal Discussion</code> <code>Ask Me Anything</code> tRNA modifications tune m<sup>6</sup>A-dependent mRNA decayCHIPseq, RNAseq, SLAMseq, XXXseq...All the seqs: Which do we use and when? Which seqs are out there? Moderator: Elene Tuesday,September\u00a030 12:30 Viro Journal Club Ruchira Monday,October\u00a006 14:30 <code>Individual Presenter</code> <code>General</code>CANCELLED Marius  Meeting in the seminar room on FLOOR 5! Date Time Type of meeting Presenter/Topic/Paper Comment Monday,May 19 14:30 <code>General</code>CANCELLED / Monday,May 26 14:30 <code>Individual Presenter</code> Joana Monday,June 02 14:30 <code>Journal Discussion</code> LINK CHANGED ON 26.05! (1)SeqTagger, a rapid and accurate tool to demultiplex direct RNA nanopore sequencing datasets ANDDemultiplexing and barcode-specific adaptive sampling for nanopore direct RNA sequencing Moderator: Erik Monday,June 09 CANCELLED WHIT MONDAY Friday,June 13 09:00 Viro Club Elene Tuesday,June 17 12:30 Viro Journal Club Ruchira Monday,June 16 CANCELLED Viro practical (Masters) Monday,June 23 CANCELLED Viro practical (Masters) Tuesday,June 24 12:30 Viro Journal Club Elene Monday,June 30 CANCELLED Dan absent Monday,July 07 14:30 <code>Ask Me Anything</code>CANCELLED Comparison of Sequencing platforms:Illumina, Nanopore &amp; PacBio Wednesday,July 09 13:00 Viro Seminar Carolin Monday,July 14 14:30 <code>Individual Presenter</code> IHW attendees practice(Caro, Elene, Pierina) talk Date Time Type of meeting Presenter/Topic/Paper Comment Monday,March 17 14:30 <code>Journal Discussion</code> MoDorado: Enhanced detection of tRNA modifications in nanopore sequencing by off-label use of modification callers Moderator:  Pierina  Monday,March 24 14:30 <code>General</code> / Dan away Monday,March 31 14:30 <code>Ask Me Anything</code> AI in our Research Monday,April 07 14:30 <code>Individual Presenter</code> Erik Jens away Monday,April 14 14:30 <code>General</code> / Dan away Monday,April 21 EASTER MONDAY Monday,April 28 14:30 <code>Individual Presenter</code> Carolin Tuesday,April 29 12:30 Viro Journal Club Elene Monday,May 05 14:30 <code>Journal Discussion</code> Immune evasion through mitochondrial transfer in the tumour microenvironmentmRNA export factors store nascent transcripts within nuclear speckles as an adaptive response to transient global inhibition of transcription Moderator:  Ruchira  Friday,May 09 09:00 Viro Club Pierina Monday,May 12 14:30 <code>Ask Me Anything</code> Methods &amp; Experiments Talk Date Time Type of meeting Presenter/Topic/Paper Comment Monday,January 20 14:30 <code>General</code> / Monday,January 27 14:30 <code>Journal Discussion</code> Charting the epitranscriptomic landscape across RNA biotypes using native RNA nanopore sequencing Moderator: Erik Monday,February 03 14:15 <code>Ask Me Anything</code> Nanopore DRS Dan has to leave at ~15:00 Tuesday,February 04 12:30 Viro Journal Club Pierina Friday,February 07 09:00 Viro Club Ruth Monday,February 10 14:30 <code>General</code> / Wednesday,February 12 13:00 Viro Seminar Hanan Monday,February 17 15:00 <code>Individual Presenter</code> Pierina Pierinas first PhD supervisor meeting Monday,February 24 14:30 <code>Journal Discussion</code> Evidence of RNA polymerase III recruitment and transcription at protein-coding gene promoters Moderator: Ruth Monday,March 03 14:00 <code>Ask Me Anything</code> RNA modifications Monday,March 10 14:30 <code>Individual Presenter</code> Dan <ol> <li>This link got changed on 26.05., because the paper was fully published. The original, preprint link was this: SeqTagger Preprint Link</li> </ol> <p>Feel free to write Erik suggestions regarding AMA topics or the Journal discussions.</p> <p></p>","tags":["Dates"]},{"location":"Home/Schedules_and_Events/monday_meeting/#the-new-monday-lab-meeting-explained","title":"The new Monday Lab meeting explained","text":"<p>Above you can find the current schedule for the AGDepledge Monday Meeting. This meeting broadly entails all members of AG Depledge, as well as individual members from other research group at the Institue of Virology (MHH). The schedule is organised by Erik - feel free to contact him in case of questions, comments, suggestions etc. This is how our weekly meetings work:</p> <ul> <li>We (try to) meet on every Monday, 14:30, in the Seminar room (J06-06).</li> <li>To keep it a bit interesting, we are rotating between 4 different formats across the weeks. These include:<ul> <li> <code>Individual Presentations</code><ul> <li>One person presents the progress of their current work. Discussions, questions and advice is encoured.</li> </ul> </li> <li> <code>Journal Discussions</code><ul> <li>Similar to a journal club, but without a single, dedicated presenter</li> <li>In advance, one paper will be chosen as the paper for the resepective Monday Lab Meeting. For that, papers can be suggested by everyone (and cleared by Dan). The chosen paper will be added to our papers-please collection and marked as such.</li> <li>As mentioned, there is no individual presenter, instead we will discuss our thoughts and questions on said paper together (for this, everybody will need to read the paper). There will however be 1-2 people (rotating) leading the discussion.</li> </ul> </li> <li> <code>AMAs (Ask Me Anything)</code> <ul> <li>An open question session dedicated to a specific topic</li> <li>In advance, one specific topic will be chosen (e.g. Nanopore DRS). Before the meeting we will give this topic some thought and collect questions we have about it/ read up on some details if we are knowledgable in that topic.</li> <li>In the meeting, we then try to awnser each others questions (obviously, often Dan will be doing a lot of the awnsering, but that is O.K.)</li> <li>Feel free to suggest specific topics you would like to discuss</li> </ul> </li> <li> <code>General meetings</code><ul> <li>General lab topics, dates etc. are discussed</li> </ul> </li> </ul> </li> <li>In case of deviations from the schedule, Erik will send out a message to inform about the changes. You can always check the current schedule at the page you are currently at</li> </ul> <p>Should you have ideas for other formats feel free to make suggestions. :)</p> <p>As we are still testing these new formats and getting a feel for them they might not yet be perfect, but we are positive we will get there!</p>","tags":["Dates"]},{"location":"Home/Schedules_and_Events/other_seminars/","title":"Other Seminars","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Home/Schedules_and_Events/upcoming_events/","title":"Upcoming Events","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/drylab_general/","title":"Overview","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Analyses_Scripts/dorado/","title":"Basecalling of Nanopore Datasets with Dorado","text":"","tags":["Nanopore","DRS","Basecalling","Dorado","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/dorado/#overview","title":"Overview","text":"<p>Basecalling is a key part of nanopore data processing. While 'live' basecalling is usually performed during nanopore runs, this is usually run in a basic (fast or high-accuracy) mode. Given that we are usually  interested in RNA modifications and poly(A) tails, re-basecalling in super-accuracy mode is often desired. The following is a guide to exactly that</p>","tags":["Nanopore","DRS","Basecalling","Dorado","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/dorado/#dorado-versions","title":"Dorado versions","text":"<p>The Dorado basecaller is periodically updated by nanopore to improve basecalling accuracy and to add new models for poly(A) tail estimation and detection of DNA/RNA modifications. You can always look for the latest version here. When a new version of Dorado is released, Dan will (usually) download and make it available in the base <code>/project/sysviro/</code> space. If the latest version is not there then  please inform Dan immediately so he can download it.</p>","tags":["Nanopore","DRS","Basecalling","Dorado","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/dorado/#basecalling-with-dorado","title":"Basecalling with Dorado","text":"<p>Invariable you will want to perform Dorado basecalling in super-accuracy mode (sup) with RNA modification detection and poly(A) tail detection enabled. This requires significant time (12-24 hours) and GPU power. For applications in which RNA mods and/or poly(A) tails are not of interest, high-accuracy mode (hac) is sufficient.</p> <p>Dorado can be launched as follows: <pre><code>/project/sysviro/dorado-0.9.0-linux-x64/bin/dorado\n</code></pre></p> <p>While there are multiple sub-arguments that can be specified, all of which are detailed here. However, the most relevant command for basecalling is 'basecaller' e.g.  <pre><code>/project/sysviro/dorado-0.9.0-linux-x64/bin/dorado basecaller\n</code></pre></p>","tags":["Nanopore","DRS","Basecalling","Dorado","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/dorado/#high-accuracy-vanilla-basecalling","title":"High-accuracy (vanilla) basecalling","text":"<p>For vanilla basecalling, the following setup is appropriate</p> <pre><code>IN=/project/sysviro/data/Nanopore-Pod5/\nOUT=/project/sysviro/data/DoradoBasecalling/\nfilename=XXX\n\n### Run basecalling (high accuracy mode, adapter trimming, BAM output)\n/project/sysviro/dorado-0.9.0-linux-x64/bin/dorado basecaller --trim adapters -r $MODEL/rna004_130bps_hac@v3.0.1 $IN/path/to/pod5/ &gt; $OUT/\"$filename\".hac.trimAdaptor.dorado.0.9.0.bam\n\n### Generate sequencing summary file \n/project/sysviro/dorado-0.9.0-linux-x64/bin/dorado summary $OUT/$filename\".hac.trimAdaptor.dorado.0.9.0.bam &gt; $OUT/$filename\".hac.trimAdaptor.dorado.0.9.0.sequencing_summary.txt\n\n### Generate FASTQ file from BAM \nmodule load BEDTools \nbamToFastq -i $OUT/$filename\".hac.trimAdaptor.dorado.0.9.0.bam -fq $OUT/$filename\".hac.trimAdaptor.dorado.0.9.0.fastq\n</code></pre>","tags":["Nanopore","DRS","Basecalling","Dorado","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/dorado/#super-accuracy-modifications-basecalling","title":"Super-accuracy (modifications) basecalling","text":"<p>The latest RNA004 super high accuracy (sup) model allows for direct detection of various RNA modifications, including poly(A) tail estimates. Here the preferred approach is to generated a single uBAM file where all modifications (inc. poly(A) tail length estimate) is calculated for each read.</p> <p>STILL NEED TO UPDATE THE BELOW TO ALLOW FOR ALL MODS + TRANSFER OF ALL TAGS TO FASTQ</p> <pre><code>dorado basecaller --trim adaptor $MODEL/rna004_130bps_hac@v3.0.1 sample.pod5 --modified-bases m6A_DRACH &gt; sample.bam\n</code></pre> <p>To go from an unaligned BAM file with modifications recorded in the MM and ML tags, it is necessary to convert to FASTQ while retaining the tags and then align to a reference using minimap2. This sounds complicated but in principal requires a single (piped) command. Note that SAMtools v1.16 or higher is needed along with minimap2 v2.26 or higher <pre><code>samtools fastq -TMM,ML ${USAM} | minimap2 -x map-ont -a -t ${num_threads} -y --secondary=no ${REFERENCE} - &gt; ${SAMFILE}\n\n#e.g. \nsamtools fastq -TMM,ML $OUT/EMC1-ARPE19-96h-1.sup-m6A.trimAdapters.dorado.0.5.1.bam | minimap2 -ax map-ont -L -p 0.99 -y --secondary=no /project/sysviro/data/reference_transcriptomes/VZV-Dumas-v2.0-complete.fasta - &gt; /project/sysviro/users/Dan/VZVm6Atest/EMC1-ARPE19-96h-1.sup-m6A.tx.sam\n</code></pre></p>","tags":["Nanopore","DRS","Basecalling","Dorado","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/","title":"Metaplot","text":"","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#overview","title":"Overview","text":"<p>Metaplots are density plots / histograms showing the clustering of sites of interest (e.g. RNA modifications) along simplified transcript models. They can be generated from datasets aligned against either the genome or transcriptome of a well annotated organism, typically the human genome (HG38). Below you will find information on how to generate different types of metaplot.</p>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#analysis-of-m6a-and-pseu-on-hg38-polya-rnas-using-r-transcriptome-level","title":"Analysis of m6A and pseU on HG38 poly(A) RNAs using R (transcriptome level)","text":"<p>input files: - modkit BED file (generated using modkit) containing modification calls against transcriptome alignments - hg38-output.txt data file which described the UTR and CDS regions of all currently annotated HG38 Ensembl transcripts (obtain from Dan or generate yourself (see below)</p> <p>The R script, txome-metaplot.R (AGDepledge/scripts/), can be used to process such an input file and produce an analysis of m6A and pseU density across sequenced RNAs. The output highlight distinct regions such as  the 5' UTR (0-1), CDS (1-2), and 3' UTR (2-3).</p> <p></p> <p></p>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#analysis-of-m6a-on-hg38-rnas-using-r-genome-level","title":"Analysis of m6A on HG38 RNAs using R (genome level)","text":"<p>input files: - latest gencode predictions (obtained via UCSC Table browser) in genepred format - reduced modkit BED files containing modification calls against genome alignments - assorted python and R scripts (AGDepledge/scripts/)</p> <p>While more complicated than analyses of transcriptome-level alignments, genome-level alignments may be preferred in many cases. The following described how to generate genome-level metaplots when using standard DRS approaches. This approach can be applied to all modifications that can be natively called by Dorado.</p>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#1-navigate-to-ucsc-table-browser","title":"1. Navigate to UCSC Table browser","text":"<ul> <li>set assembly to Dec. 2013 HG38</li> <li>set track to All Gencode V47 (or later version)</li> <li>set table to Basic(wgEncodeGencodeBasicV47)</li> <li>set output file name to hg38_gencode_v47.genePred</li> <li>download</li> </ul>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#2-make-annotation-and-region_sizes-files-this-creates-a-very-large-master-annotation-file-bed-format-of-every-nucleotide-in-the-transcriptome-then-sort-and-remove-unsorted-version-before-generation-a-region_sizes-file-the-region_sizes-file-catalogs-the-transcriptomic-coordinates-of-the-start-and-end-sites-of-the-transcript-regions-ie-5utr-cds-and-3utr-note-that-this-step-only-needs-to-be-performed-once-and-the-only-reason-to-generate-a-new-version-is-when-a-later-gencode-annotation-version-is-desired","title":"2. Make annotation and region_sizes files - this creates a very large master annotation file (bed format) of every nucleotide in the transcriptome. Then sort and remove unsorted version before generation a region_sizes file. The region_sizes file catalogs the transcriptomic coordinates of the start and end sites of the transcript regions (i.e. 5\u2019UTR, CDS and 3\u2019UTR). Note that this step only needs to be performed once and the only reason to generate a new version is when a later gencode annotation version is desired.","text":"<pre><code>    python make_annot_bed.py --genomeDir /project/sysviro/data/reference_genomes/Homo_sapiens/UCSC/hg38/Sequence/Chromosomes/ --genePred hg38_gencode_v47.genePred &gt; hg38_annot_gencode_v47.bed\n\n    sort -k1,1 -k2,2n hg38_annot_gencode_v47.bed &gt; hg38_annot_gencode_v47.sorted.bed\n\n    rm hg38_annot_gencode_v47.bed\n\n    python size_of_cds_utrs.py --annot hg38_annot_gencode_v47.sorted.bed &gt; gencode_v47_region_sizes.txt\n</code></pre>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#3-generate-and-parse-intersect-files-needed-first-we-prepare-a-limited-bed6-format-bed-file-generated-from-the-original-modkit-output-then-we-intersect-this-with-the-annotation-file-generated-in-step-2","title":"3. Generate and parse intersect files needed. First we prepare a limited BED6 format BED file generated from the original modkit output. Then we intersect this with the annotation file generated in step 2.","text":"<pre><code>    module load bedtools\n\n    cut -f1,2,3,4,5,11 ../HG38/NHDF_DMSO_48h_1.sup-m6A_DRACH.trimAdapters.dorado.0.9.0.hg38.sorted.bed &gt; m6A.sorted.bed\n\n    intersectBed -a m6A.sorted.bed -b hg38_annot.sorted.bed -sorted -wo -s &gt; annot_m6a.sorted.bed\n\n    # Filter outputs for desired depth and frequency\n    awk '$5 &gt; 50 &amp;&amp; $6 &gt; 10' annot_m6a.sorted.bed &gt; annot_m6a.filtdepth50freq10.sorted.bed\n</code></pre>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#4-generate-metagene-co-ordinate-file","title":"4. Generate metagene co-ordinate file","text":"<pre><code>    python rel_and_abs_dist_calc.py --bed annot_m6a.filtdepth50freq10.sorted.bed --regions region_sizes.txt &gt; m6a.filtdepth50freq10.dist.measures.txt\n</code></pre>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#5-plotting","title":"5. Plotting","text":"<p>The output distance measures file can be loaded into the metaplot-HG38-genome.R script for plotting. This script can be modified as needed to add additional tracks.</p> <p></p>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#analysis-of-m6a-and-pseu-on-pol-iii-derived-rnas-using-r-genome-level","title":"Analysis of m6A and pseU on Pol III-derived RNAs using R (genome level)","text":"<p>input files: - modkit BED files containing modification calls, generated from running bedtools intersect with Pol III tx and tRNAs with genome-level alignments (generated using modkit) - pol-III-ome-metaplot.R (AGDepledge/scripts/)</p> <p>In this example we seperate tRNAs and other Pol III RNAs for the metaplot analysis. Note that as these are non-coding RNAs, we do not include any region information (e.g. there is no CDS region). You can adjust the R script to accept as many input files as you like but you will need to make some manual updated to specify which should be plotted.</p> <p></p>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/metaplot/#references","title":"References","text":"<p>Much of this section was inspired by and adapted from metaplotR </p>","tags":["Metaplot","Bash","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/ninetails/","title":"Ninetails","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>","tags":["Nanopore","DRS","Ninetails","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/ninetails/#what-is-ninetails","title":"What is Ninetails?","text":"<p>Ninetails is a R-package that can identify non-A nucleotides (C, G, U) in poly(A)-tails and their position on signal level. Find the paper here. Their GitHub page can be found here. Eriks JournalClub presentation can be found in the AGDepledge Drive. </p>","tags":["Nanopore","DRS","Ninetails","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/ninetails/#installation","title":"Installation","text":"<p>In principle, the installation guide can be found here, but some additional steps might/might not need to be taken. Because of this it is advised to follow the instructions below first. Ninetails could theoretically be run on different devices, but the installation proves to be difficult in some cases (although it is quite straight forward, once you know how to). Different options are listed below. Erik recommends using Ninetails via an HPC interactive RStudio session. </p> Recommended: MHH-HPC - interactive RStudio [Ubuntu]Local on own PC - Docker (RStudio server) [Windows]Locally PC (MHH or own) in RStudio [Windows]MHH-HPC - R [GNU-Linux] <p>This works. If you only want to use Ninetails (and not install it) you can ignore the RStudio container section (as this has already been done, see <code>/project/sysviro/R/01_RStudio/</code>).  The HPC has two ways to use an interactive RStudio session, 'Simple' and 'Advanced'.  Do not try to install Ninetails in a Simple interactive RStudio Session.It will only bring you misery.  Additionally, Erik strongly advises against trying to install the additional requirements manually (as described in the Ninetails Wiki on GitHub) and instead use conda. Using conda should eliminate the need to install any of these additional requirements manually. </p> <p>This works. (But it's slower and less convenient than running it on the HPC.) If you want to install this on a MHH-PC you will ned admin permissions (Erik hasn't tried this route).</p> <ol> <li>Download and install docker</li> <li>Use the docker terminal to get the Ninetails image: <pre><code>docker pull ghcr.io/nemitheasura/ninetails-docker:latest\n</code></pre></li> <li>You can see your available images with: <pre><code>docker image list\n</code></pre> This also shows you your images ImageID.</li> <li>Run: (1) <pre><code>docker run -it -p 8787:8787 -e PASSWORD=123 --rm --mount type=bind.src=&lt;/path/to/local/direc/you/want/to/access&gt;,dst=/home/rstudio r-ninetails\n</code></pre></li> <li>Enter <code>localhost:8787</code> in your browser and login with username: rstudio and password:123.</li> <li>In RStudio, run: <pre><code># necessary after every restart\nreticulate::use_condaenv(\"r-ninetails\")\n</code></pre></li> <li>Ninetails should now be operational. You can use the build in test data set and this command to verify: <pre><code>results &lt;- ninetails::check_tails(\nnanopolish = system.file('extdata',\n                        'test_data',\n                        'nanopolish_output.tsv',\n                        package = 'ninetails'),\nsequencing_summary = system.file('extdata',\n                                'test_data',\n                                'sequencing_summary.txt',\n                                package = 'ninetails'),\nworkspace = system.file('extdata',\n                        'test_data',\n                        'basecalled_fast5',\n                        package = 'ninetails'),\nnum_cores = 2, # Use number of cores applicable to your device. N-1 recommended.\nbasecall_group = 'Basecall_1D_000',\npass_only=TRUE,\nsave_dir = '~')\n</code></pre></li> </ol> <p>This works. (But it's slower and less convenient than running it on the HPC.) You do not need admin permissions to install this on an MHH-PC.</p> <ol> <li>Install conda <pre><code>curl https://repo.anaconda.com/archive/Anaconda3-2024.10-1-Windows-x86_64.exe --output .\\Downloads\\Anaconda3-2024.10-1-Windows-x86_64.exe\n</code></pre> Run the exe and follow the installing instructions. If you install on an MHH-PC you will need to install for your user only (unless you have admin privileges).</li> <li>Start the Anaconda Prompt and look for your installation path <pre><code>where conda\n</code></pre> Copy that path for step 5.</li> <li>Build the Ninetails-conda environment Get this YAML-file (not the same as the available file on GitHub, this one has been modified to fit to Windows): r-ninetails_windows.yml<pre><code>name: r-ninetails\nchannels:\n- conda-forge\n- defaults\ndependencies:\n- bzip2=1.0.8\n- ca-certificates=2022.12.7\n- libffi=3.4.2\n- libsqlite=3.40.0\n- openssl=3.0.7\n- pip=22.3.1\n- python=3.8.15\n- setuptools=65.6.3\n- tk=8.6.12\n- wheel=0.38.4\n- zlib=1.2.13\n- pip:\n    - absl-py==1.4.0\n    - astunparse==1.6.3\n    - cachetools==5.2.1\n    - certifi==2022.12.7\n    - charset-normalizer==3.0.1\n    - flatbuffers==23.1.4\n    - gast==0.4.0\n    - google-auth==2.16.0\n    - google-auth-oauthlib==0.4.6\n    - google-pasta==0.2.0\n    - grpcio==1.51.1\n    - h5py==3.7.0\n    - idna==3.4\n    - importlib-metadata==6.0.0\n    - keras==2.11.0\n    - libclang==15.0.6.1\n    - markdown==3.4.1\n    - markupsafe==2.1.1\n    - numpy==1.24.1\n    - oauthlib==3.2.2\n    - opt-einsum==3.3.0\n    - packaging==23.0\n    - protobuf==3.19.6\n    - pyasn1==0.4.8\n    - pyasn1-modules==0.2.8\n    - requests==2.28.2\n    - requests-oauthlib==1.3.1\n    - rsa==4.9\n    - six==1.16.0\n    - tensorboard==2.11.2\n    - tensorboard-data-server==0.6.1\n    - tensorboard-plugin-wit==1.8.1\n    - tensorflow==2.11.0\n    - tensorflow-estimator==2.11.0\n    - tensorflow-io-gcs-filesystem==0.29.0\n    - termcolor==2.2.0\n    - typing-extensions==4.4.0\n    - urllib3==1.26.14\n    - werkzeug==2.2.2\n    - wrapt==1.14.1\n    - zipp==3.11.0\n</code></pre> And this command: <pre><code>conda env create -f r-ninetails_windows.yml\n</code></pre></li> <li>(Close R, then) Download and install Rtools for your required R version from here.</li> <li>Setup Ninetails in R in R: <pre><code># Install required packages\nif(!requireNamespace(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nif(!requireNamespace(\"devtools\", quietly = TRUE))\n    install.packages(\"devtools\")\nif(!requireNamespace(\"reticulate\", quietly = TRUE))\n    install.packages(\"reticulate\")\nif(!requireNamespace(\"rhdf5\", quietly = TRUE))\n    BiocManager::install('rhdf5')\nif(!requireNamespace(\"rhdf5\", quietly = TRUE))\n    devtools::install_github('LRB-IIMCB/ninetails')\n\n# activate conda environment (necessary after every restart)\nSys.setenv(RETICULATE_CONDA=\"/path/to/conda.bat\")\nreticulate::use_condaenv('r-ninetails')\n</code></pre></li> <li>Ninetails should now be operational. You can use the build in test data set and this command to verify: <pre><code>results &lt;- ninetails::check_tails(\nnanopolish = system.file('extdata',\n                        'test_data',\n                        'nanopolish_output.tsv',\n                        package = 'ninetails'),\nsequencing_summary = system.file('extdata',\n                                'test_data',\n                                'sequencing_summary.txt',\n                                package = 'ninetails'),\nworkspace = system.file('extdata',\n                        'test_data',\n                        'basecalled_fast5',\n                        package = 'ninetails'),\nnum_cores = 2, # Use number of cores applicable to your device. N-1 recommended.\nbasecall_group = 'Basecall_1D_000',\npass_only=TRUE,\nsave_dir = '~')\n</code></pre></li> </ol> <p>Probably doesn't work at the moment (mainly because 'zlib1g-dev' (which seems to be mandatory) is missing in the included extensions for the available R modules), but would be super nice.</p> <ol> <li>In case that doesn't work try <pre><code>docker run -it -p 8787:8787 -e PASSWORD=123 --rm --mount type=bind.src=&lt;/path/to/local/direc/you/want/to/access&gt;,dst=/home/rstudio &lt;ImageID&gt;\n</code></pre> </li> </ol>","tags":["Nanopore","DRS","Ninetails","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/ninetails/#setting-up-the-rstudio-container","title":"Setting up the RStudio container","text":"<p>On the HPC, do the following:</p> <ol> <li> <p>Create RStudio Container settings file with the following content (adjustments can be made if needed): ninetails_RStudio_container.def<pre><code># Select Container Template\nBootstrap: docker\nFrom: rocker/rstudio:4.4\n\n%post\n# Create Important HPC Directory Environment\nmkdir -p /leinesw\nmkdir -p /software\nmkdir -p /project\nmkdir -p /etc/ssh\n\n# Install Packages\napt update &amp;&amp; apt install -y python3 python3-pip python3-venv libhdf5-dev zlib1g-dev\n\n# Install R Modules\nRscript -e \"install.packages('BiocManager')\"\nRscript -e \"install.packages('devtools')\"\nRscript -e \"BiocManager::install('rhdf5')\"\nRscript -e \"devtools::install_github('LRB-IIMCB/ninetails')\"\n\n%labels\nAuthor: Erik Fuhrmann\nVersion: 1.0\nR-Version: 4.4\n</code></pre></p> </li> <li> <p>Build container <pre><code>apptainer build ninetails_RStudio_container.sif ninetails_RStudio_container.def\n</code></pre></p> </li> </ol>","tags":["Nanopore","DRS","Ninetails","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/ninetails/#setting-up-the-conda-environment","title":"Setting up the conda environment","text":"<pre><code>wget https://raw.githubusercontent.com/LRB-IIMCB/ninetails_processing/refs/heads/main/ninetails_docker_build/r-ninetails.yml\nconda env create -f r-ninetails.yml\n</code></pre>","tags":["Nanopore","DRS","Ninetails","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/ninetails/#running-the-interactive-rstudio-session","title":"Running the interactive RStudio session","text":"<ol> <li>Visit the MHH OnDemand site</li> <li>Click on 'CPU - Software' - 'RStudio - Advanced'</li> <li>Enter '/project/sysviro/R/01_R_Studio/ninetails_RStudio_container.sif' (or an alternative build if you would like to use another) for path</li> <li>Decide a password</li> <li>Fill out the specs.<ol> <li>Cores: Ninetails parallelises with cores which makes them the main bottleneck, so more cores = faster. Keep in mind however that the interactive RStudio session runs on the leinecpu_interactive partition. This partition is also used for interactive terminal sessions and doesn't have too many cores in total (96). 16 cores is reasonable.</li> <li>Memory: Unless you only want to look at a subset of data, 64GB is reasonable. 32GB will probably already run you out of memory. [Note: 64GB might still not be enough]</li> <li>Job Time: One run will take up to ~5 hours. 12 hours is reasonable.</li> </ol> </li> <li>Click Launch and connect to the session. Type your username and the password you have decided on.</li> <li>In R: <pre><code># activate conda environment (necessary after every restart)\nSys.setenv(RETICULATE_CONDA=\"/project/usr/apps/software/zen2/software/Anaconda3/2022.05/bin/conda\")\nreticulate::use_condaenv('r-ninetails')\n</code></pre></li> <li>Ninetails should now be operational. You can use the build in test data set and this command to verify: <pre><code>results &lt;- ninetails::check_tails(\nnanopolish = system.file('extdata',\n                        'test_data',\n                        'nanopolish_output.tsv',\n                        package = 'ninetails'),\nsequencing_summary = system.file('extdata',\n                                'test_data',\n                                'sequencing_summary.txt',\n                                package = 'ninetails'),\nworkspace = system.file('extdata',\n                        'test_data',\n                        'basecalled_fast5',\n                        package = 'ninetails'),\nnum_cores = 2, # Use number of cores applicable to your device. N-1 recommended.\nbasecall_group = 'Basecall_1D_000',\npass_only=TRUE,\nsave_dir = '~')\n</code></pre></li> </ol>","tags":["Nanopore","DRS","Ninetails","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/ninetails/#pipeline","title":"Pipeline","text":"<p>The postprocessing section is WIP. Check here for more info.</p> <pre><code>---\ntitle: Ninetails data workflow\n---\nflowchart TD\n    A{SQK-RNA002 data&lt;br&gt;.fast5s} ---|basecalling&lt;br&gt;Guppy, pre v.6.3.2,&lt;br&gt;--fast5_out!| B[ ]:::empty;\n    subgraph Guppy output;\n    C[.fastqs];\n    D[output .fast5s];\n    E[sequence_summary.txt];\n    end;\n    B --&gt; C;\n    B --&gt; D;\n    B --&gt; E;\n    C  --&gt;|cat| F[.fastq];\n    subgraph reference\n    G{.fasta};\n    H(.fai);\n    end;\n    G --&gt;|samtools&lt;br&gt;faidx| H;\n    E --- I[ ]:::empty;\n    G --- I;\n    H -.- I;\n    F --- K[ ]:::empty;\n    G --- K[ ]:::empty;\n    H -.- K[ ]:::empty;\n    K ---|minimap2/samtools&lt;br&gt;align, filter, sort, index| L[ ]:::empty;\n    subgraph alignment data\n    M[.sorted.bam];\n    N(.bai);\n    end;\n    L ---&gt; M;\n    L ---&gt; N;\n    F --- O[ ]:::empty;\n    G --- O[ ]:::empty;\n    H -.- O[ ]:::empty;\n    J -.- O[ ]:::empty;\n    M --- O[ ]:::empty;\n    N -.- O[ ]:::empty;\n    I --&gt;|nanopolish&lt;br&gt;index| J(.index.*);\n    O --&gt;|nanopolish&lt;br&gt;polya| P[polya .tsv];\n    D --- Q[ ]:::empty;\n    E --- Q[ ]:::empty;\n    P --- Q[ ]:::empty;\n    Q --&gt;|ninetails&lt;br&gt;check tails| R[output tables];\n    R --&gt;|post processing| S[WIP];\n    classDef empty width:1px,height:1px,fill:transparent,stroke:none;</code></pre> <p>Associated scripts can be obtained by demand (ask Erik).</p>","tags":["Nanopore","DRS","Ninetails","R"]},{"location":"How-to_Dry-lab/Analyses_Scripts/wdx/","title":"Multiplexed Nanopore direct RNA Sequencing with WarpDemux","text":"","tags":["WarpDemuX","DRS","Nanopore","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/wdx/#overview","title":"Overview","text":"<p>WarpDemuX (WDX) is an ultra-fast and high-accuracy adapter-barcoding and demultiplexing tool for Nanopore direct RNA sequencing. It currently provides support for RNA004 chemistry, specifically poly(A) DRS and nano-tRNASeq.</p>","tags":["WarpDemuX","DRS","Nanopore","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/wdx/#installation","title":"Installation","text":"<p>Running WarpDemuX requires setting up a conda environment and installing required packages. Below provides a general overview on how to do this but please check the WarpDemuX Github before doing so in case changes are made to the procedure.</p> <pre><code># Switch to an interactive node, you will likely need &gt;16Gb of memory for the installation\nsrun --partition=leinecpu_interactive --job-name=\"wdx\" --cpus-per-task=1 --mem-per-cpu=32G -t8:00:0 --pty /bin/bash\n\n# Navigate to \ncd /project/sysviro/tools\n\n# Grab latest version using Git (optional, usually the version available in /project/sysviro/tools/WarpDemuX is the one to use)\nmodule load git\ngit clone --recursive https://github.com/KleistLab/WarpDemuX.git\n\n# Set up a new conda environment (this is installed into your home directory)\nmodule load Anaconda3/2023.03-1\nmodule load GCC\n\nconda env create -n WDX -f WarpDemuX/environment.yml\n\nconda activate WDX\n\npip install -e WarpDemuX/\n\nconda deactivate WDX\n</code></pre> <p></p>","tags":["WarpDemuX","DRS","Nanopore","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/wdx/#running-wdx","title":"Running WDX","text":"<p>While WDX comprises two stages (preparation, prediction), typical usage is to run these together via the demux command</p> <pre><code>module load Anaconda3/2023.03-1\n\nconda activate WDX\n\nwarpdemux demux -i /project/sysviro/data/Nanopore-Pod5/P2_WDX-6/Multiplexed2_STM2457_treatment_HEVandVZV/20250415_1926_P2S-01949-A_PBC47053_0d982d23/pod5/ -o /project/sysviro/data/Nanopore-Pod5/P2_WDX-6/Multiplexed2_STM2457_treatment_HEVandVZV/20250415_1926_P2S-01949-A_PBC47053_0d982d23/ -m WDX4_rna004_v0_4_4 -j 20 --batch_size_output 40000 --save_boundaries true\n\nconda deactivate WDX\n</code></pre> <p>Of critical importance is selecting the correct model (-m). The WarpDemux Github has a list of all available models and the barcodes and chemistries associated with each. So, if you are demultiplexing a run using RNA004 chemistry and the WDX barcodes 03, 05, and 07, you would choose model WDX4_rna004_v0_4_4. </p> <p>!!!Note that current RNA004 models are limited to a maximum of four barcodes in a run. If you multiplex to greater depths than this then we will need to generate a new model with the help of the original developers!!!</p> <p></p>","tags":["WarpDemuX","DRS","Nanopore","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/wdx/#wdx-output-parsing","title":"WDX output &amp; parsing","text":"<p>Following the demux step, an output folder will be generated in the output directory. The folder name will reflect the model used and the date of the demux run. The output folder will contain a  subfolder called predictions which contains large number of .csv files, each file containing predictions for 40000 reads. These should be combined into a single file <pre><code>cat barcode_predictions_* &gt; barcode_predictions.all.csv\n</code></pre> The barcode_predictions.all.csv contains a list of all read ids (column 1), the predicted barcode (col 2), the confidence score (col 3), and the individual confidence scores for each barcode (cols 4+). <pre><code>read_id,predicted_barcode,confidence_score,p03,p04,p05,p07,p-1\ncd3c0eab-1e24-4059-86e5-0f5845d417d4,5,0.893,0.045,0.0047,0.9384,0.0107,0.0012\n4cc652e6-2369-452d-ad5d-668ed5b63198,5,0.964,0.0048,0.0006,0.9786,0.0141,0.0019\n58346cc3-6624-4b59-97cb-0ab91c361ad9,5,0.976,0.0077,0.0037,0.9838,0.0036,0.0011\n021ff088-6183-4035-9d84-8d5c7c52cfe0,4,0.994,0.002,0.9961,0.0005,0.0008,0.0006\nd1a12590-4408-4ce4-a740-0696bf6f5651,4,0.991,0.0033,0.9944,0.0006,0.0011,0.0006\n87ce2169-e6c0-4d5b-a7b5-c39df46264a8,5,0.972,0.0046,0.0015,0.982,0.0098,0.0021\n09dde30c-5aa6-452c-a0ea-178efe165dba,4,0.966,0.0112,0.9767,0.002,0.0078,0.0024\n79bf882c-f6f1-405a-9672-60d76691f72f,5,1.0,0.0,0.0,0.9998,0.0001,0.0001\n33bc3975-8fdd-4583-abad-0094bafba834,4,0.997,0.0009,0.998,0.0004,0.0004,0.0004\n</code></pre> We process this using the following commands to select only high confidence barcodes to produce a list of read ids associated with each demultiplexed sample. In the below example we are compiling lists of read ids associated with barcode 04 and barcode 05 and with probability scores of &gt;90% <pre><code>awk -F',' '$2 == 4 &amp;&amp; $3 &gt;= 0.9 {print $1}' predictions_filtered.bam.csv &gt; barcode04.p0.9.list.txt\nawk -F',' '$2 == 5 &amp;&amp; $3 &gt;= 0.9 {print $1}' predictions_filtered.bam.csv &gt; barcode05.p0.9.list.txt\n</code></pre></p>","tags":["WarpDemuX","DRS","Nanopore","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/wdx/#filtering-of-dorado-basecalled-ubams","title":"Filtering of Dorado-basecalled uBAMs","text":"<p>Finally, we can use BBMap to create individual (demultiplexed) uBAMs from a dorado-basecalled uBAM that is generated using the full datasets (multiplexed). <pre><code>module load BBMap\nmodule load SAMtools\nfilterbyname.sh in=../../EMC1_wt_U1mut_ARPE19_MPlex-2.hac.trimAdapters.dorado.0.8.0.bam out=EMC1_wt_ARPE19-wdx4.hac.trimAdapters.dorado.0.8.0.bam names=EMC1_wt_U1mut_ARPE19_MPlex-2.WDX12_rna004-v0.4.4/barcode04.p0.9.list.txt include=true overwrite=true usejni=t -Xmx16g\n</code></pre></p> <p>From here, we can proceed to generating FASTQ files for downstream alignments etc.</p>","tags":["WarpDemuX","DRS","Nanopore","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/alignments/","title":"Overview","text":"<p>A sequence read (or read-pair) may have one or more primary alignments against a reference genome. </p> <p>A single primary alignment is the most common outcome but where multiple primary alignments occur, one will (usually randomly) be set as the primary alignment while the remainder will be flagged as secondary alignments. These are usually all flagged with a mapping quality (mapQ) score of 0.</p> <p>Secondary alignments are common in low complexity regions and/or repeat regions or when structural variation is present. For instance, reads aligning against the terminal repeats of VZV or HSV-1 will usually have two equally good alignments, one of which will be set as the 'primary'.</p> <p>Supplementary alignment occurs when two non-overlapping sections of a sequence read align to distinct locations. These are most commonly seen where structural variation is present</p>","tags":["Alignment"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/fun_with_flags/","title":"Fun with (alignment) flags","text":"","tags":["Alignment","Flags","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/fun_with_flags/#overview","title":"Overview","text":"<p>Filtering sequence read alignments based on their alignment flag (SAM/BAM column 3) is extremely useful for evaluating the mapping quality of read alignments and inferring strandedness. It is also possible to filter  SAM/BAM files to retain only alignments with specific alignment flags.</p> <p></p> <p>*** flags relevant to nanopore DRS and derivatives</p>","tags":["Alignment","Flags","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/fun_with_flags/#nanopore-considerations","title":"Nanopore considerations","text":"<p>Nanopore datasets comprise single-end reads. A common step in processing nanopore datasets is to retain only primary alignments for downstream processing. This can be accomplished when converting between SAM to BAM e.g.</p> <p><code>samtools view -F 2308 -o filtered.bam in.sam</code></p> <p>The sum total of nanopore-related flags is 0+4+16+256+2048 = 2324. Subtracting 0+16 (primary alignments against forward and reverse strand) gives 2308. The -F flag acts to exclude all bits that can be used to make 2308 (e.g. 4+256+2048).</p> <p>Filtering for reverse strand alignments only can thus be accomplished with -f 16 (primary only) or -f 2320 (primary, secondary, and supplementary)</p> <p>Filtering for primary forward strand alignments only requires -F 2324</p>","tags":["Alignment","Flags","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/fun_with_flags/#illumina-considerations","title":"Illumina considerations","text":"<p>Illumina datasets may be unpaired (single-end, one read per fragment) or paired-end (two reads per fragment). For unpaired reads, the rules outlined above for nanopore remain valid. For paired-end reads, alignment flag operations are a little more complex.</p> <p>Examples: - a correctly paired primary alignment with R1 aligning to the forward strand would have a value of 0+1+2+32+64 = 99 - a correctly paired primary alignment with R2 aligning to the forward strand would have a value of 0+1+2+32+128 = 163 - an incorrectly paired primary alignment with R2 aligning to the reverse strand could have a value of 1+16+128 = 145 (implies mate is aligned to same strand)</p>","tags":["Alignment","Flags","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/fun_with_flags/#further-reading","title":"Further reading","text":"<p>Alignment flag decoder</p>","tags":["Alignment","Flags","Bash"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/pseudoalignment/","title":"Performing pseudoalignment with kallisto","text":"<p>Pseudoalignment is a rapid, k-mer-based computational method for RNA-seq and metagenomic data analysis that identifies which reference transcripts a read is compatible with, rather than finding its exact, base-by-base alignment location. It breaks reads into k-mers and maps them against a transcriptome de Bruijn graph to quickly find the set of transcripts that match the read. This provides sufficient accuracy for quantifying gene expression while being much faster than traditional alignment. </p>","tags":["alignment","pseudoalignment","kallisto"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/pseudoalignment/#key-aspects-of-pseudoalignment-include","title":"Key aspects of pseudoalignment include:","text":"<ul> <li>Speed/Efficiency: It skips the time-consuming process of generating base-to-base alignments, often allowing millions of reads to be processed in minutes on standard computers</li> <li>Method: Pseudoalignment is designed to handle non-uniquely mapped reads by identifying the potential transcripts of origin, which can then be further refined using algorithms like Expectation-Maximization. </li> <li>Tools: Primary tools utilizing this method include Kallisto, Salmon, and Sailfish. See here for a detailed description of how pseudoalignment works</li> <li>Applications: It is primarily used for RNA-seq transcript abundance quantification and is only suitable for use with high-resolution transcriptome annotations (e.g. Gencode Human/Mouse etc)</li> <li>Limitations: Because it does not provide exact positional information (e.g., specific splice sites or exact base mismatches), it is not suitable for applications requiring high-precision mapping</li> </ul>","tags":["alignment","pseudoalignment","kallisto"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/pseudoalignment/#standard-pseudoalignment-code","title":"Standard pseudoalignment code","text":"<p>The code snippet below will allow you to quickly align paired-end RNASeq data. Note that input fastq files should not be preprocessed (i.e. no adapter or quality trimming).  See script_templates folder in sysviro for a SLURM compatible example.  <pre><code># define variables\nFASTA=/project/sysviro/data/reference_transcriptomes/kallisto/gencode.v47.transcripts.fa\nREF=/project/sysviro/data/reference_transcriptomes/kallisto/gencode.v47.transcripts\n\n# index reference transcriptome (note: this only needs to be performed once)\n/project/sysviro/bin/kallisto index $REF\n\n/project/sysviro/bin/kallisto quant -i $REF -o $OUT -b 100 --bias --rf-stranded $IN_1.fq.gz $IN_2.fq.gz\n</code></pre></p> <p>-b 100  :  bootstrapping is a computational technique used to estimate the technical variance and uncertainty in transcript abundance quantification  --bias  :  enables a model that learns the empirical fragment length distribution and fragment start position distribution from the data to improve estimation  --fr-stranded : Read 1 aligns to the for strand of the transcript, Read 2 aligns to the rev strand (i.e. first read = sense). Typical for ligation-based protocols  --rf-stranded : Read 1 aligns to the rev strand of the transcript, Read 2 aligns to the for strand (i.e. first read = antisense). Typical for dUTP based methods (DNBSEQ, NEB Ultra II Directional)   Note: It is absolutely critical to pay attention to which stranded flag is being used!!! For unstranded data, these flags should be omitted. If you are at all unsure about whether your data is stranded or not then read the stranded section and run RSeQC </p>","tags":["alignment","pseudoalignment","kallisto"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/pseudoalignment/#choice-of-reference-transcriptome","title":"Choice of reference transcriptome","text":"<p>The optimal choice of (human) reference transcriptome is Gencode. We currently use the v47 assembly but later versions are now available. The Comprehensive gene annotation for the CHR regions is sufficient and the GTF file can be downloaded using wget i.e. by right-clicking on the GTF link and selecting 'copy link' and pasting this after wget e.g.  <pre><code>wget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/gencode.v47.annotation.gtf.gz\nwget https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_47/gencode.v47.transcripts.fa.gz\n\ngunzip gencode.v47.annotation.gtf.gz\ngunzip gencode.v47.transcripts.fa.gz\n</code></pre></p>","tags":["alignment","pseudoalignment","kallisto"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/pseudoalignment/#converting-transcript-counts-into-gene-counts","title":"Converting transcript counts into gene counts","text":"<p>Given that most downstream applications involve differential gene expression analysis using DeSeq2 or edgeR, it is usually desirable to convert the reported transcript-level counts produced by kallisto into gene-level counts. This is especially useful for pathway analyses as most programs do not accept transcript IDs. The R snippet below should be sufficient to merge the kallisto outputs from all subfolders (samples) within a folder and to produce a gene-level counts file that is ready for input into e.g. DeSeq2.</p> <p><pre><code>library(tximport)\nlibrary(rhdf5)\nlibrary(rtracklayer)\n\n### SET WORKING DIRECTORY ### You will need to edit this and direct it your downloaded kallisto folder\nsetwd(\"C:/Users/depledgd/Dropbox/RNASeq/kallisto\")\n\n### IMPORT ANNOTATION ### Import gencode v47 annotation (the same GTF file used for the pseudoalignment)\nlibrary(rtracklayer)\ngtf &lt;- import(\"../gencode.v47.annotation.gtf\") \ngtf &lt;- gtf[gtf$type == \"transcript\"]\nt2g &lt;- unique(data.frame(tx = gtf$transcript_id, gene = gtf$gene_id))\n\n### USE TXIMPORT TO SUMMARIZE TRANSCRIPT COUNTS INTO GENE COUNTS\n## For multiple samples, each named as a folder in the kallisto directory (can be abundance.h5 or abundance.tsv file). \naccessions &lt;- list.dirs(full.names=FALSE)[-1]\nkallisto.dir&lt;-paste0(accessions)\nkallisto.files&lt;-file.path(kallisto.dir,\"abundance.tsv\")\nnames(kallisto.files)&lt;- accessions\ntx.kallisto &lt;- tximport(kallisto.files, type = \"kallisto\", tx2gene = t2g, countsFromAbundance =\"no\", ignoreAfterBar=TRUE)\n\n### GENERATE TWO COLUMN OUTPUT FORMAT\ncounts&lt;-as.data.frame(tx.kallisto$counts)\n\n### ROUND VALUES (DESEQ2 DOES NOT LIKE FRACTIONS), AND WRITE TO OUTPUT FILE\nwrite.table(round(counts), \"allcounts.txt\", row.names = TRUE, quote = FALSE, sep = \"\\t\")\n</code></pre> The output should be a text file containing all gene names in the first column with respective counts for each sample shown in the additional columns</p>","tags":["alignment","pseudoalignment","kallisto"]},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/stranded/","title":"Stranded(ness)","text":""},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/stranded/#strandedness-in-short-read-rna-seq-experiments","title":"Strandedness in (short-read) RNA-Seq experiments","text":"<p>Strandedness in RNA-seq describes whether the sequencing protocol preserves information about which DNA strand the original RNA transcript came from. In stranded (directional) libraries, read orientation can be used to distinguish sense from antisense transcription, improving quantification accuracy for overlapping genes and isoforms. In unstranded libraries, this information is lost, and reads are compatible with transcripts on either strand. Correctly specifying strandedness during analysis is essential, as using the wrong setting can bias expression estimates, particularly for antisense-overlapping genes and at the transcript level.</p>"},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/stranded/#a-warning","title":"A Warning!!!","text":"<p>While strandedness is broadly classified as forward-stranded or reverse-stranded, the many different tools available have played fast and loose with these terms which can cause significant confusion. Below is a table highlighting the strandedness parameters for tools commonly used in the lab. For a comprehensive list, take a look here</p> Tool RF/fr-firststrand stranded (dUTP) FR/fr-secondstrand stranded (Ligation) Unstranded IGV (5p to 3p read orientation) F2R1 F1R2 F2R1 or F1R2 TopHat (--library-type parameter) fr-firststrand fr-secondstrand fr-unstranded HISAT2 (--rna-strandness parameter) R/RF F/FR NONE STAR n/a (STAR doesn\u2019t use library strandedness info for mapping) NONE NONE Picard CollectRnaSeqMetrics (STRAND_SPECIFICITY parameter) SECOND_READ_TRANSCRIPTION_STRAND FIRST_READ_TRANSCRIPTION_STRAND NONE Kallisto quant (parameter) --rf-stranded --fr-stranded NONE FeatureCounts (-s parameter) 2 1 0 Example methods/kits: dUTP, NSR, NNSR, Illumina TruSeq Strand Specific Total RNA, NEBNext Ultra II Directional, Watchmaker RNA Library Prep Kit with Polaris Depletion Ligation, Standard SOLiD, NuGEN Encore, 10X 5\u2019 scRNA data Standard Illumina, NuGEN OvationV2, SMARTer universal low input RNA kit (TaKara), GDC normalized TCGA data"},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/stranded/#easy-ways-to-infer-strandedness","title":"Easy ways to infer strandedness","text":"<p>RSeQC (An RNA-seq Quality Control Package) provides a number of useful modules that can comprehensively evaluate high throughput sequence data especially RNA-seq data. Some basic modules quickly inspect sequence quality, nucleotide composition bias, PCR bias and GC bias, while RNA-seq specific modules evaluate sequencing saturation, mapped reads distribution, coverage uniformity, strand specificity, transcript level RNA integrity etc.</p>"},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/stranded/#setting-up-and-running-rseqc-to-infer-strandedness","title":"Setting up and running RSeQC to infer strandedness","text":"<p>RSeQC includes a python script for determining strandedness. To do this, it is easiest to subsample 10,000-20,000 paired-reads from the fastq files of one of your samples. These should be aligned with STAR or HiSat2 to the target genome (not transcriptome) and parsed to a BAM file. A corresponding BED12 file containing the gene annotations should also be created or downloaded. Together, these can be fed to the infer_experiment script. </p> <pre><code># install using pip\npip3 install RSeQC\n\ninfer_experiment.py -i aligned.bam -r genes.bed\n</code></pre>"},{"location":"How-to_Dry-lab/Analyses_Scripts/Read_Alignment_%26_Processing/stranded/#interpreting-the-output","title":"Interpreting the output","text":"<p>A typical output would look something like  <pre><code>This is PairEnd Data\nFraction of reads failed to determine: 0.02\n\nFraction of reads explained by:\n1++,1--,2+-,2-+ : 0.91\n1+-,1-+,2++,2-- : 0.07\n</code></pre> For paired-end data:  1++,1--,2+-,2-+ dominant \u2192 reverse-stranded \u2192 kallisto --rf-stranded  1+-,1-+,2++,2-- dominant \u2192 forward-stranded \u2192 kallisto --fr-stranded  If neither wins decisively (i.e. if  both patterns are ~0.5), strandedness was not preserved </p>"},{"location":"How-to_Dry-lab/Other_tools/AcademicCloud/accloud_general/","title":"Overview","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <p>The AcademicCloud is an online service hosted by the Society for Scientific Data Processing G\u00f6ttingen (GWDG). As members of participating universities (e.g. the MHH), we have access to the service provides through AcademicCloud - for free. Each employee or student automatically also has an AcademicCloud account. Services include, but are not limited to, Chat AIs like  ChatGTP-4, 50 GB of cloud space, GitLab, LaTeX editors etc. It's very helpful, so give it a try!  In order to login, visit https://academiccloud.de and click on Login. Then click on federated Login and select the MHH. In the upcoming window, enter your MHH credentials. The initial login may take a few minutes.</p>","tags":["Miscellaneous"]},{"location":"How-to_Dry-lab/Other_tools/AcademicCloud/furthertools_1/","title":"TheothertoolsTODO","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_tools/AcademicCloud/owncloud/","title":"OwnCloud","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_tools/Git/git/","title":"Git","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_tools/Git/gitlab/","title":"GitLab (& GitHub)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_tools/Python/language_python/","title":"General","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_tools/Python/packages_python/","title":"Useful (Bioinformatics) Packages","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_tools/R/language_r/","title":"General","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_tools/R/packages_r/","title":"Useful (Bioinformatics) Packages","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./","title":"Erik\u2019s Code Snippets To Aid and Support You (E.C.S.T.A.S.Y.)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <p>Hello there!</p> <p>Every once in a while one finds themselves in need of code. Not rarely one needs larger scripts or entire programs. If you have come here in search for this, you are at the wrong place. In fact, this place is quite the opposite! This place is meant to be a (unordered) collection of (miscellaneous) code snippets and pieces I have created/collected at some point that might (or might not) be useful. If you are looking for proper pipelines and scripts, check out the How to Dry lab - Analyses section. If you are looking for general HPC/bash commands, check out How to Dry lab - Work with the HPC - Useful Commands. Have fun!  ~Erik  </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#hpcbash-related-stuff","title":"HPC/bash related stuff","text":"","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#calculate-average-mean-qs-from-a-bam-file-by-a-list-of-reads","title":"Calculate average (mean) QS from a bam file by a list of reads","text":"<p><pre><code>samtools view XXX.bam |\\\n grep -f list_of_read_idsXXX.txt - |\\\n sed 's/.*qs:.:\\([0-9]*\\.\\?[0-9]*\\).*/\\1/' |\\\n awk '{total += 10^(-$1/10)} END {print -10*log(total/NR)/log(10)}'\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#get-number-of-softclipping-events-5-and-3-from-bam-file-by-softclipping-length","title":"Get number of softclipping events (5' and 3') from bam file by softclipping length","text":"<p>For both leading and trailing softclipping per read: <pre><code>samtools view XXX.bam |\\\ncut -f6 |\\\n grep 'S' |\\\n sed -r 's/(^[0-9]+S)?([0-9]+[NMDIH=X])+([0-9]+S$)?/\\1 \\3/' |\\\n sort -b -t'S' -k1,1n -k2,2n |\\\n uniq -c\n</code></pre></p> <p>For leading and trailing softclipping independently: <pre><code>samtools view XXX.bam |\\\n cut -f6 |\\\n grep 'S' |\\\n sed -r 's/(^[0-9]+S)?([0-9]+[NMDIH=X])+([0-9]+S$)?/\\1\\n\\3/' |\\\n sort -n |\\\n uniq -c\n</code></pre></p> <p>For leading + trailing softlipping per read: [TODO] </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#carrying-a-specific-or-multiple-sam-tag-eg-pti-from-the-ubam-over-to-the-aligned-sambam","title":"Carrying a specific (or multiple) SAM tag (e.g. pt:i:) from the uBAM over to the aligned SAM/BAM","text":"<p><pre><code># Use -T flag to write comments into the, fastq file; use -y to carry them over to SAM/BAM\n# In case of mutliple tags just list them comma separated\nsamtools fastq -Tpt,MM,ML $BAM_DIREC/\"$NAME\".\"$MODNAME\".dorado.\"$DORVER\".bam | \\\nminimap2 -ax splice -k14 -uf -y --secondary=no $HYB_REF_GX - &gt; \\\n$OUT/\"$NAME\".\"$MODNAME\".dorado.\"$DORVER\".\"$HYB_NAME\".sam\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#get-polya-tail-length-distribution-and-read_id-reference-from-a-bam-file","title":"Get poly(A)-tail length distribution (and read_id, reference) from a bam file","text":"<pre><code>awk '{col_pt=\"\"; for(i=12;i&lt;=NF;i++){if($i~/^pt/){col_pt=$i; break}}; sub(/pt:i:/,\"\",col_pt); print $1,$3,col_pt}' &lt;(samtools view XXX.bam)\n</code></pre> <p>Note</p> <p>You could also use a more straight forward approach with <code>grep</code>, but this would exclude reads without the pt:i: tag (which is the case for reads dorado couldn't determine the poly(A)-tail length), which might be inconvenient.</p> <p></p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#get-the-number-of-multiple-modifications-from-a-bam-file-order-in-which-they-appearwith-probability-filtering-9804-per-read-in-addition-to-read-id-sequence-length-and-polya-tail-length","title":"Get the number of (multiple) modifications from a bam file (order in which they appear)(with probability filtering (&gt;98.04%)) per read in addition to read id, sequence length and poly(A)-tail length","text":"<p><pre><code>awk '{col_ml=\"\"; col_pt=\"\"; for(i=12;i&lt;=NF;i++){if($i~/^pt/){col_pt=$i}; if($i~/^MM/){col_mm=$i}; if($i~/^ML/){col_ml=$i}}; sub(/pt:i:/,\"\",col_pt); n_mod=split(col_mm, arr_mm, \";\")-1; sub(/ML:B:C,/,\"\",col_ml); n_ml=split(col_ml, arr_ml, \",\"); for(j=1;j&lt;=n_mod;j++){ n_entr[j]=split(arr_mm[j], b, \",\")-1}; start=1; end=n_entr[1]; for(k in n_entr){end=start+n_entr[k]-1; amount[k]=0; for(l=start;l&lt;=end;l++){if(arr_ml[l]&gt;250){amount[k]++}}; start=start+n_entr[k]}; printf \"%s %s %s %s \", $1, $3, length($10), col_pt; for(m in amount){printf \"%s \", amount[m]}; print \"\" }'\n</code></pre> As awk multi-line: <pre><code>{\n    col_ml = \"\"\n    col_pt = \"\"\n    # Find col_pt, col_mm, col_ml in fields &gt;= 12\n    for (i = 12; i &lt;= NF; i++) {\n        if ($i ~ /^pt/) { col_pt = $i }\n        if ($i ~ /^MM/) { col_mm = $i }\n        if ($i ~ /^ML/) { col_ml = $i }\n    }\n    # Clean up strings\n    sub(/pt:i:/, \"\", col_pt)\n    n_mod = split(col_mm, arr_mm, \";\") - 1\n    sub(/ML:B:C,/, \"\", col_ml)\n    n_ml = split(col_ml, arr_ml, \",\")\n\n    # Find number of entries for each modification\n    for (j = 1; j &lt;= n_mod; j++) {\n        n_entr[j] = split(arr_mm[j], b, \",\") - 1\n    }\n\n    start = 1\n    end = n_entr[1]\n\n    # For each modification, count how many ML elements are &gt;250 (&gt;98.04%)\n    for (k in n_entr) {\n        end = start + n_entr[k] - 1\n        amount[k] = 0\n        for (l = start; l &lt;= end; l++) {\n            if (arr_ml[l] &gt; 250) {\n                amount[k]++\n            }\n        }\n        start = start + n_entr[k]\n    }\n\n    # Print output\n    printf \"%s %s %s %s \", $1, $3, length($10), col_pt\n    for (m in amount) {\n        printf \"%s \", amount[m]\n    }\n    print \"\"\n}\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#get-all-reads-from-a-bam-file-with-the-help-of-a-dorado-sequencing-summary-file-originating-from-one-specific-pod5-file-and-within-a-specific-polya-tail-length-range","title":"Get all reads from a bam file (with the help of a dorado sequencing summary file) originating from one specific pod5 file and within a specific poly(A)-tail length range","text":"<p><pre><code># Here: pod5 nr. 100 from pass as an example; poly(A)-tail range 50-200\nawk '$1~/.*pass.*100.pod5/{print $2}' XXX.sequencing_summary.txt |\\\n grep -f - &lt;(samtools view XXX.bam) |\\\n grep \"pt:i:\" |\\\n awk '{polyA=\"\"; polyA=int(gensub(/.*pt:i:([0-9]*).*/, \"\\\\1\", \"g\")); if(polyA&gt;=50 &amp;&amp; polyA&lt;=200){print $1,$3,polyA}}' - &gt;\\\n XXX.txt\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#get-the-number-of-bases-at-the-last-seq-position-across-a-bam-file","title":"Get the number of bases at the last SEQ position across a bam file","text":"<p><pre><code>samtools view XXX.bam |\\\n cut -f 10 |\\\n sed -r 's/.*(.$)/\\1/g' |\\\n sort |\\\n uniq -c\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#get-the-distribution-of-polysame-base-at-end-of-seq-across-bam-eg-a","title":"Get the distribution of poly(same-base) at end of SEQ across bam (e.g. A)","text":"<p><pre><code>samtools view XXX.bam |\\\n cut -f 10 |\\\n sed -r -n 's/(.*[^A])(A+$)/\\2/p' |\\\n sort |\\\n uniq -c\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#extract-polya-tail-trace-information-from-the-dorado-verbose-log-file","title":"Extract poly(A)-tail trace information from the dorado verbose log file","text":"<p><pre><code># Generate verbose dorado basecalling log\n$DORVERBIN basecaller -r $DORMOD --estimate-poly-a -vv $IN/$NAME/ &gt; \\\n$BAM_DIREC/\"$NAME\".\"$MODNAME\".dorado.\"$DORVER\".bam 2&gt; \\\n$OUT/\"$NAME\".\"$MODNAME\".dorado.\"$DORVER\".vlog\n\n# Get corresponding lines from the file\nawk '/^.{26}\\[trace\\] [a-f0-9]{8}-[a-f0-9\\-]{27} PolyA/' XXX.vlog\n\n# Extract relevant data to table\nawk 'BEGIN {print \"read_id polya_bases signal_anchor signal_range_start signal_range_end signal_length samples_per_base trim read_len\"} /^.{26}\\[trace\\] [a-f0-9]{8}-[a-f0-9\\-]{27} PolyA/ {id=gensub(/^.{34}([a-z0-9-]{36}).*/,\"\\\\1\",\"g\"); sub(/^.{83}/,\"\"); gsub(/[^[:digit:] \\.]/,\"\"); gsub(/  +/,\" \"); print id,$0}' XXX.vlog &gt;\\\n XXX.txt\n</code></pre> As an awk multi-line: <pre><code>BEGIN {\n    print \"read_id polya_bases signal_anchor signal_range_start signal_range_end signal_length samples_per_base trim read_len\"\n}\n\n# Get lines containing the poly(A) trace information\n/^.{26}$trace$ [a-f0-9]{8}-[a-f0-9\\-]{27} PolyA/ {\n    # Extract the read id\n    id = gensub(/^.{34}([a-z0-9-]{36}).*/, \"\\\\1\", \"g\")\n\n    # Remove the first 83 characters from the current record\n    sub(/^.{83}/, \"\")\n\n    # Remove all non-digit, non-dot, non-space characters\n    gsub(/[^[:digit:] \\.]/, \"\")\n\n    # Replace sequences of multiple spaces with a single space\n    gsub(/  +/, \" \")\n\n    # Print the id followed by the cleaned data\n    print id, $0\n}\n</code></pre></p> <p>Note</p> <p>Since dorado v1.0.0 this shouldn't be necessary anymore because the poly(A)-tail tracing information is now placed under its own flag in the uSAM file. (Similarly, extracting the poly(A)-tail length via <code>pt:i:</code> can instead be obtained from the sequencing summary file.)</p> <p></p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#get-overview-of-a-pod5-file","title":"Get overview of a pod5 file","text":"<p><pre><code># Create conda environment and install the pod5 package, then run:\npod5 inspect summary XXX.pod5\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#extract-important-stats-polya-tail-length-modifications-here-filter-of-9804-from-a-bam-file-and-list-of-ids-here-99-barcode-confidence-by-read_id-and-multiplex-groups-warpdemux","title":"Extract important stats (poly(A)-tail length, modifications (here: filter of &gt;98.04%)) from a bam file (and list of ids; here: 99% barcode confidence) by read_id and multiplex groups (WarpDemuX)","text":"<p><pre><code>awk 'BEGIN {print \"read_id barcode reference seq_length polyA_len n_inosine n_m6A n_pseU\"} FNR==NR {a[$1]=$2; next}; $1 in a {col_ml=\"\";col_pt=\"NA\"; for(i=12;i&lt;=NF;i++){if($i~/^pt/){col_pt=$i}; if($i~/^MM/){col_mm=$i}; if($i~/^ML/){col_ml=$i}}; sub(/pt:i:/,\"\",col_pt); n_mod=split(col_mm, arr_mm, \";\")-1; sub(/ML:B:C,/,\"\",col_ml); n_ml=split(col_ml, arr_ml, \",\"); for(j=1;j&lt;=n_mod;j++){ n_entr[j]=split(arr_mm[j], b, \",\")-1}; start=1; end=n_entr[1]; for(k in n_entr){end=start+n_entr[k]-1; amount[k]=0; for(l=start;l&lt;=end;l++){if(arr_ml[l]&gt;250){amount[k]++}}; start=start+n_entr[k]}; printf \"%s %s %s %s %s \",$1,a[$1],$3,length($10),col_pt; for(m in amount){printf \"%s \", amount[m]}; print \"\"}' id_sample_keytable-filt0.99.txt &lt;(samtools view XXX.bam) &gt; XXX.txt\n</code></pre> As an awk multi-line: <pre><code>WIPWIPWIPWIPWIP\n</code></pre></p> <p>Warning</p> <p>In this iteration the modification names are not extracted automatically and instead have to be added manually, for this, the order of modification appearance in the bam file has to be checked (see the relevant MM/ML tags in the SAM/BAM file).  This could be automated if the order of the tags gets parsed first and copied to the output file headers (or translated to proper names).</p> <p></p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#old-method-to-open-a-rstudio-session-running-on-the-hpc-new-use-ondemand","title":"Old method to open a RStudio session running on the hpc (New: Use OnDemand)","text":"<p><pre><code>cd /project/sysviro/R/01_R_Studio\nsbatch startrstudio.sbatch\ncat rstudio-server.&lt;user&gt;.out # get user, password and adress (type into browser) from here\n\nscancel -f &lt;jobID&gt; # to end the session (do not forget!)\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#r-related-stuff","title":"R related stuff","text":"","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#transform-ascii-qual-qs-sam-format-to-decimal","title":"Transform ASCII QUAL QS (SAM format) to decimal","text":"<p><pre><code># Edit string first to add escape backslashes, e.g. here:\n# https://www.browserling.com/tools/add-slashes\n\n# Transform ASCII\nlibrary(stringr)\nqual &lt;- \"ASCIISTRING\"\nqual &lt;- str_split_1(qual, \"\")\nqual_num &lt;- list()\nqual_prob &lt;- list()\nfor (i in 1:length(qual)) {\n    qual_num &lt;- append(qual_num, (as.integer(charToRaw(qual[i])) - 33))\n    qual_prob &lt;- append(qual_prob, 10^(-qual_num[[i]]/10))\n}\nqual_num &lt;- unlist(qual_num) # Numeric Phred QS\nqual_prob &lt;- unlist(qual_prob) # Probability\n\n# Example: Calculate mean QS\n-10*log10((mean(qual_prob)))\n</code></pre> </p>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#ms-word-related-stuff","title":"MS Word related stuff","text":"","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/E.C.S.T.A.S.Y./#placing-a-space-every-third-letter-in-a-continuous-string-eg-dna-sequence","title":"Placing a space every third letter in a continuous string (e.g. DNA sequence)","text":"<ol> <li>Search (Ctrl+F)  Replace (small dropdown menu)  More  Find what: <code>^1</code>  Replace with nothing (removes newlines)</li> <li>Find what: <code>([A-Z]{3})</code>  Replace with <code>\\1</code> (includes a space!) </li> </ol>","tags":["Miscellaneous","Bash","Python","R"]},{"location":"How-to_Dry-lab/Other_useful_code/code_XY/","title":"Code 2","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/awk/","title":"Parsing documents (awk, grep & Co)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/bash/","title":"How to use the Terminal & Bash","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/commandline_tools/","title":"Other useful command line tools","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/commands/","title":"Useful Commands","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <p>This is a collection of general HPC/bash related commands. If you are looking for something more specific, check out the respective Pipeline &amp; Script section or here.</p> <p> </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#starting-an-interactive-cpu-session","title":"Starting an interactive (CPU-)session","text":"<pre><code>srun --partition=leinecpu_interactive --job-name=\"insert_name_here\" --cpus-per-task=1 --mem-per-cpu=1G -t4:00:0 --pty /bin/bash\n</code></pre> <p>To change the memory, number of cores or time change the parameters accordingly.</p> <p>Note</p> <p>You might still stumble across a very similar command in some place containing <code>--partition=interactive</code> (that doesn't work). This is the old name of the partition and is no longer used.</p> <p></p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#starting-an-interactive-gpu-session","title":"Starting an interactive (GPU-)session","text":"<p><pre><code>srun --partition=leinegpu_interactive --job-name=\"insert_name_here\" --cpus-per-task=1 --mem-per-cpu=1G -t4:00:0 --pty /bin/bash\n</code></pre> </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#get-stats-on-run-job","title":"Get stats on run job","text":"<pre><code># (format can be changed, only examples are being shown)\n# job that already ran\nseff &lt;jobID&gt;\n\n# job that is still running\nsstat &lt;jobID&gt;.batch --format=JobID,MaxRSS\n\n# detailed list on jobs from a specific time\nsacct --starttime YYYY-MM-DD --format=User,JobID,Jobname,partition,state,time,start,end,elapsed,AveRSS,MaxRss,MaxVMSize,nnodes,ncpus,nodelis\n</code></pre> <p>Tip</p> <p>For a more graphical and user-friendly overview you can check out (active) jobs at the MHH OnDemand site at jobs \u2192 expand job \u2192 detailed metrics</p> <p></p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#check-current-hpc-resources-in-detail","title":"Check current hpc resources in detail","text":"<p><pre><code>sinfo -o \"%.13n %.20P %.14G %.5a %.9T %.10e %.8m %.4c %.15C\"\n</code></pre> </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#check-hpc-partition-details","title":"Check hpc partition details","text":"<p><pre><code>scontrol show partition\n</code></pre> </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#observe-gpu-nvidia-card-usage-of-a-running-job-live","title":"Observe GPU (NVIDIA card) usage of a running job live","text":"<p><pre><code>watch -n 1 srun --jobid=&lt;jobID&gt; nvidia-smi\n</code></pre> (Cannot be run in an interactive session.) </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#open-process-manager","title":"Open process manager","text":"<p><pre><code>htop\n</code></pre> </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#get-total-disk-usage-of-a-project-folder","title":"Get total disk usage of a project folder","text":"<pre><code>df -h /project/sysviro\n</code></pre>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#get-storage-usage-of-current-directory-recursiveexcluding-backups","title":"Get storage usage of current directory (recursive)(excluding backups)","text":"<p><pre><code>du -hLlxs . --exclude \"./.snapshots*\" 2&gt;/dev/null # total\ndu -hd1 . --exclude \"./.snapshots*\" 2&gt;/dev/null # subdirectories + total\n</code></pre> </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/commands/#get-a-list-of-all-users-in-a-project-folder","title":"Get a list of all users in a project folder","text":"<p><pre><code>getent group gs-mhh-hpc-sysviro\n</code></pre> </p>","tags":["Guide","Bash"]},{"location":"How-to_Dry-lab/hpc/how_to_move_files/","title":"How to move, copy and rename files","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/hpc_general/","title":"Overview","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <p>Whenever we do Science\u2122 we often find ourselves with data on our hands. Often, these data also want to be anlysed in one of many different ways. Sometimes, this works perfectly fine with our institute issued computers or our trusty laptops. But occasionally (or frequently, depending on what you are working on) this just doesn't cut it. The calculations are too slow, data too large, software too difficult to install and results annoying to share. But do not fret! Because this is exactly what the HPC is here for you!  Like many other institutions, the MHH too has it's own High Perfomance Computing (HPC) Cluster, called 'leine' (named after the river Leine that runs through Hannover). This allows you to run computationally heavy tasks quickly and in the background on the server, instead of a local machine. Applications can reach from sequence basecalling, to analysing single-cell data in sever-provided RStudio sessions, up to training ML models.  The following pages give you a bit of an overview on how the HPC works and how to effectively utilise it.</p>"},{"location":"How-to_Dry-lab/hpc/infrastructure/","title":"HPC Hardware","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/modules/","title":"Modules","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/restore_backup/","title":"Restoring deleted files from backup","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Dry-lab/hpc/slurm/","title":"How to run background jobs (Slurm)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"How-to_Wet-lab/wetlab-general/","title":"Welcome to the lab!","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <p>Welcome to the exciting life of the wet-lab!</p> <p>Before you getgoing, please make yourself familiar with the provided instructions and guidelines in this section.  To look at our established protocols, see the Protocols sections..</p>","tags":["Miscellaneous"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/","title":"Welcome to the lab!","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/#what-you-need-to-know","title":"What you need to know","text":"","tags":["Instruction"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/#labcoats","title":"Labcoats","text":"<ul> <li>S1 labcoats can be found in the cupboard next to the secretary office in the hallway</li> <li>S2 labcoats can be found in the cupboard next to the scullery</li> </ul>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/#waste-disposal","title":"Waste disposal","text":"<ul> <li> <p>Normal lab waste containers</p> <ul> <li>Do not overfill, the lid still needs to fit</li> <li>Fold the bag shut, take it to the scullery, put a bit of water in the bottom and put it in the autoclave</li> </ul> </li> <li> <p>Chemical waste in the S1 chemical hood</p> <ul> <li>Liquid waste in the chemical hood: See <code>AGDepledge:\\Protocols\\Waste containers.docx</code> for </li> <li>Solid waste: Goes in the plastic bag after evaporation, tied shut and put in the blue chemical bin next to the hood. Do not close until full.</li> </ul> </li> <li> <p>S2 waste containers vs. solid waste</p> <ul> <li>See <code>AGDepledge:\\Protocols\\Golden Rules for Disposal of Cell Culture Waste.docx</code></li> <li>Solid waste: Use the S2 buckets (they have lids with a bigger edge). Bags need to be closed with tape. Do not overfill! Take the bucket to the scullery, put a bit of water in the botoom and put it in the autoclave.</li> </ul> </li> <li> <p>Normal waste and recycling</p> <ul> <li>See <code>AGDepledge:\\Protocols\\Waste-management.pdf</code></li> </ul> </li> </ul>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/#stock-room-for-plastics","title":"Stock Room for plastics","text":"<ul> <li>The stock room for plastics (tips, pipettes, dishes etc.) can be found in the rear hallway</li> <li>The chemiclas (Ethanol, Methanol, Isopropanol etc.) are stored in room 2242 (key required)</li> </ul>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/#storing-samples-in-80c-and-liquid-nitrogen","title":"Storing samples in -80\u00b0C and liquid nitrogen","text":"<ul> <li>Label items and boxes</li> <li>Add them to the database:<ul> <li><code>AGDepledge:\\Databases\\-80C freezer database.xlsx</code> or</li> <li><code>AGDepledge:\\Databases\\Liquid nitrogen tank cell storage</code></li> </ul> </li> </ul>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/#training-required-equipment","title":"Training required - Equipment","text":"<ul> <li>If you want to use a piece of equipment and you don't know how, check if there is a note on or near it to see who to speak to.</li> <li>Otherwise ask Dan, Ruth, Michael or Jens Bohne (who is also the manager for biological safety)</li> </ul>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Guidelines/wetlab-guide-intro/#how-to-leine-agdepeldge-drive","title":"How to Leine &amp; AGDepeldge-Drive","text":"<ul> <li>To see how to use Leine and the AGDepledge drive, and how to store/backup data, see <code>AGDepledge:\\Bioinformatics resources\\Intro to HPC course</code></li> <li>Also check out this Wiki for further information</li> </ul>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Protocols/cell_cuture_table/","title":"Golden Rules for the disposal of Cell culture waste","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> Surface area (cm<sup>2</sup>) Seeding density<sup>1</sup> Cells at confluency<sup>2</sup> Versene (mL of 0.05% EDTA). Approx. volume Trypsin (mL of 0.05% trypsin, 0.53 mM EDTA). Approx. volume Growth medium (mL). Approx. volume Dishes 35mm 8.8 0.3x10<sup>6</sup> 1.2x10<sup>6</sup> 1 1 2 60mm 21.5 0.8x10<sup>6</sup> 3.2x10<sup>6</sup> 3 3 5 100mm 56.7 2.2x10<sup>6</sup> 8.8x10<sup>6</sup> 5 5 12 150mm 145 5.0x10^^ 20.0x10<sup>6</sup> 10 10 30 Culture plates 6-well 9.6 0.3x10<sup>6</sup> 1.2x10<sup>6</sup> 1 1 1 to 3 12-well 3.5 0.1x10<sup>6</sup> 0.5x10<sup>6</sup> 0.4 to 1 0.4 to 1 1 to 2 24-well 1.9 0.05x10<sup>6</sup> 0.24x10<sup>6</sup> 0.2 to 0.3 0.2 to 0.3 0.5 to 1.0 48-well 1.1 0.03x10<sup>6</sup> 0.12x10<sup>6</sup> 0.1 to 0.2 0.1 to 0.2 0.2 to 0.4 96-well 0.32 0.01x10<sup>6</sup> 0.04x10<sup>6</sup> 0.05 to 0.1 0.05 to 0.1 0.1 to 0.2 Flasks T-25 25 0.7x10<sup>6</sup> 2.8x10<sup>6</sup> 3 3 3-5 T-75 75 2.1x10<sup>6</sup> 8.4x10<sup>6</sup> 5 5 8-15 T-175 175 4.9x10<sup>6</sup> 23.3x10<sup>6</sup> 17 17 35-53 T-225 225 6.3x10<sup>6</sup> 30x10<sup>6</sup> 22 22 45-68 <ol> <li> <p>Seeding density is given for each culture vessel type as follows: Dishes and Flasks: Cells per vessel; Culture plates: Cells per well.\u00a0\u21a9</p> </li> <li> <p>The number of cells on a confluent plate, dish, or flask will vary with cell type. For this table, HeLa cells were used.\u00a0\u21a9</p> </li> </ol>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/cell_cuture_waste/","title":"Golden Rules for the disposal of Cell culture waste","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <ul> <li>After working in the hood, check whether you should dispose of the small waste bag in the hood and put it in the big waste bucket.</li> <li>If you are working with viruses, put all the waste (tubes, pipette tips, etc.) in the small waste bag.</li> <li>If the bag/bucket is 75% full dispose of it!</li> <li>Tape the bag up before taking it out of the hood. </li> <li>For the big waste bucket, add some water to the bucket tape the bag shut and put it in the autoclave at once. If the autoclave is already running you can leave it in the lab.</li> <li>For liquid waste: use the vacuum pump to suck up any waste. If you want to pour your flasks into a falcon tube first then suck up the waste from the falcon tube at the end when you are finished working in the hood.</li> <li>The vacuum pump has to have some korsolex-extra in the bottom of the bottle before you start using it.</li> <li>Do not overfill the vacuum pump bottle.</li> <li>When 75% full the bottle needs to be autoclaved, there is a spare bottle that can be put in place. Place the bottle lid closed in an S2 waste bucket to carry it over to the scullery.</li> <li>Open the lid of the bottle slightly before autoclaving</li> </ul>","tags":["Instruction","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/first_aid_phenol-trizol/","title":"First aid Emergency procedure for phenol/trizol","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <p>IF SWALLOWED, DO NOT INDUCE VOMITING... RATHER RINSE YOUR MOUTH WITH WATER. AFTERWARDS DRINK WATER IN SMALL SIPS.</p> <p>PEG-400 can be found on the chemical shelf above lab benches.</p> <code>EYE:</code> IF this product COMES INTO CONTACT with the eyes: <ul> <li>Immediately hold the eyelids open if possible and flush intermittenly for 5 mins with fresh running water, rinse with PEG solution and repeat for as long as needed.</li> <li>Ensure complete irrigation of the eye by keeping eyelids apart and away from the eye.</li> <li>Transport to hospital without delay.</li> <li>Removal of contact lenses after an eye injury should only be undertaken by skilled personnel.</li> </ul> <code>SKIN:</code> Contamination of the skin with Phenol and some of its derivatives may produce rapid collapse.  BEWARE \u2026 standard first aid treatment for chemical burns (washing the skin with cold water for 10-15 mins) may increase systemic absorption and toxicity in the case of Phenol burns \u2026 THUS: <p>IF SPLASHED on skin: </p> <ul> <li>Wearing gloves, remove contaminated clothing and swab area repeatedly with Polyethylene Glycol 400 (PEG) for 20 mins. Rinse with water afterwards.</li> <li>Cover lightly with sterile dressing.</li> <li>Treat for shock if required.</li> <li>Call 112 or bring affected person to the emergency room.</li> </ul>","tags":["Instruction"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/","title":"An Introduction to Cell Culture","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p> <p>The ability to grow cells in vitro has been central to biomedical discovery for the past century. While cells in a dish have adapted to their new environment and are not identical to cells in vivo, many of the basic cellular processes remain intact. </p> <p>Primary cells are cells taken directly from an animal or tissue and put into a dish.  Cells are classified as a cell line as they are passaged in vitro such that they become more homogenous, with the cells with the highest growth potential predominating.  Cell strains are derived from cell lines that have undergone selection process. </p>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/#sterile-technique","title":"Sterile Technique","text":"<p>The biggest challenge to cell culture is keeping cells from becoming contaminated with fungus or bacteria.  The nice nutrient-rich media provided to the cells is very attractive the microbes in the air and on our hands.  Wear gloves at all times.   Always work with cells in the tissue culture hoods.  These hoods provide filtered air and reduce potential contamination.  Before beginning work in the hood, be sure to spray down all surfaces (including your gloved hands) with 70% ethanol.  Minimize things coming in and out of the hood, and be sure to spray down any bottles or equipment coming in and out with 70% ethanol.  Open and close dishes carefully, and leave caps on bottles as much as possible. If you work with viruses, use incidin (for enveloped viruses) or optisept (for non-enveloped viruses) - make sure you spray the bottles &amp; racks when you take them out of the hood and disinfect the hood bench after you\u2019re done working. </p>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/#media","title":"Media","text":"<p>Different cell types have different nutritional, hormonal, and adhesion requirements in order to be grown in vitro.  Most cell lines, are adhesive cells that will grow right on plastic dishes with no special extracellular matrix being provided for it.  Most cells are grown in DMEM \u2013 Dulbecco/Vogt Modified Eagle\u2019s Minimal Essential Media but other media such as RPMI, DMEM/F12 or specialized keratinocyte media for example can also be used.  It contains amino acids, salts, glucose, and vitamins, and iron, and we add sodium pyruvate, L-glutamine, Pen-Strep and usually 8% fetal bovine serum is added (aliquots in freezer). Keep the media in the fridge. Make sure to label them with your initials, date, and whether or not they have any supplements. Unlabeled bottles will be discarded. All unopened media bottles of our group are stored in the 4\u00b0C room (Room #2280).</p>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/#tissue-culture-hood-calendar","title":"Tissue culture hood calendar","text":"<p>Make sure you book the hood for the duration you need it as early as possible. If you see on the calendar that no one is going to use it after you, close it down and make sure that all the other devices in the lab \u2013 microscope, centrifuge, water bath, cell counter are turned off &amp; all the lights are switched off.</p>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/#thawing-cells","title":"Thawing Cells","text":"<p>Cells are frozen in either media containing a higher percentage of fetal bovine serum or pure serum with 5-15% DMSO.  The DMSO is important for lowering the freezing temperature of the cells, which allows for a slower freezing process.  This slow freeze keeps ice crystals from forming and puncturing the cell membrane.  The DMSO can also be toxic to the cells, though, so they should be removed from the media with DMSO quickly.  </p>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/#thawing-cells-cont","title":"Thawing Cells (Cont)","text":"<ol> <li>Take cells from the liquid nitrogen and put them into a water bath. </li> <li>The cells should be shaken gently within the water bath to expedite thawing (the goal is to thaw the cells within a minute or so).</li> <li>Immediate dilute the cells in pre-warmed media in a 15 ml conical tube.</li> <li>Put the tube in the centrifuge the cells at 1200 rpm for 5-15 minutes.  (Make sure the centrifuge is balanced!)</li> <li>Aspirate off the media containing the DMSO, being careful to not disturb the cell pellet at the bottom.</li> <li>Re-suspend the cells in an appropriate amount of media.</li> <li>Plate the cells in the appropriate dish.  </li> <li>Gently push the cells back and forth and then left and right in order to ensure even distribution of the cells.</li> <li>Place the cells in an incubator at 37 degrees with a CO<sub>2</sub> level of 5%. Note: Some companies/cells may require seeding of cells with DMSO and removing the diluted DMSO the next day rather than spinning the cells first.</li> </ol>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/#passaging-cells","title":"Passaging Cells","text":"<p>Once cells become confluent, many are growth inhibited and their general physiology can be altered.  Most cell lines should be passaged once the cells reach confluence so that they do not become quiescent, and will continue dividing and growing.   Adherent cells need to be dissociated from the dish.  Trypsin is an enzyme that will lyse the proteins that connect the cells.  It works best at 37\u00b0C and is inactivated by the serum in media.  Write the cell type, passage number, your initials and date on the dishes and freezing vials when freezing. Preferably cells should not be continuously passaged for more then 30-35 passages depending on the cell type. Primary cells usually cannot be passaged more then 15-20 times.  </p> <ol> <li>Remove the growth media.</li> <li>Wash the cells once with PBS to remove any remaining serum.</li> <li>Add 1ml of 0.5% trypsin with EDTA to a 10 cm dish of cells.</li> <li>Put the dish into the incubator for 1-5 minutes.  (Different cell types will take different amounts of time to be trysinized.)</li> <li>After one-three minutes, check the cells under a microscope to see that they are rounded. Shaking the dish should cause the cells to detach and float.</li> <li>Add media with serum (approximately 9 ml).</li> <li>Dilute the cells by adding 1-2 ml of the trypsinized cells into new dishes containing 8-9 ml of fresh media. Alternatively, count the cells using the Countess and seed a specific amount in desired vessel. Check useful cell numbers for cell culture in protocol folder for estimates of seeding amount for different vessels.</li> </ol>","tags":["Protocol","Cell Culture"]},{"location":"How-to_Wet-lab/Protocols/intro_cell_culture/#freezing-cells","title":"Freezing Cells","text":"<p>Most cells can be frozen in fetal bovine serum with 10% DMSO.  As explained in the thawing section, DMSO lowers the freezing temperature of the cells, slowing freezing and helping prevent ice crystals.  The cells should be put in the freezer as soon as they are in contact with the DMSO, though, because the DMSO can be toxic to them.  The vial of cells should also be put in a Mr. Frosty freezing container to further slow the freezing process. Usually freeze around 1x10<sup>6</sup> cells per vial.</p> <ol> <li>Follow steps 1-6 in the Passaging Cells section of the protocol.</li> <li>Put one plate of detached cells into a 15 ml conical.</li> <li>Centrifuge the cells for 3 minutes at 1200 rpm.</li> <li>Aspirate the media, being careful to not disturb the cell pellet.</li> <li>Resuspend pellet in appropriate amount (0.5-1x10<sup>6</sup> cells/ml) FCS containing 10% DMSO or 10% DMSO, 30% FCS in media.</li> <li>Put the 1.5 ml of cells into a freezing vial.</li> <li>Wrap the tube in paper towel and put into a Styrofoam container or other freezing container.</li> <li>Put the container in a -70 freezer for 1 day.</li> <li>Cells should then be moved to a liquid nitrogen tank for long-term storage.</li> </ol>","tags":["Protocol","Cell Culture"]},{"location":"Miscellaneous/Helpful_MHH_LinksResources/mhh_res_1/","title":"Links 1","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Miscellaneous/Helpful_externel_Resources/external_res_1/","title":"Helpful external Links","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>","tags":["Website"]},{"location":"Miscellaneous/Helpful_externel_Resources/external_res_1/#depledge-lab-website","title":"Depledge Lab Website","text":"Resource Template documentation Template website Template repo Greene Lab website Greene Lab repo Testbed website Testbed markdown Other websites with the same template YAML syntax checker <p>When editing the website repo, I recommend pressing \".\". it opens the integrated VSCode for GitHub, makes it a lot easier. It might also help to make changes on the testing_branch and create a pull request. That way not every change is imeadiatly pushed to main and can be previewed. See more</p>","tags":["Website"]},{"location":"Miscellaneous/Templates/template_1/","title":"Auget fuit nymphis paludibus","text":"","tags":["LoremIpsum"]},{"location":"Miscellaneous/Templates/template_1/#miletida-rogantem-suos-poteras-rhoetei-verte-erat","title":"Miletida rogantem suos poteras Rhoetei verte erat","text":"<p>Lorem markdownum gerit quotiensque fuit alieni, equidem credere. Lapsum timor [rabiemque dat] haec edita dea sanguis ducis capiti: inprobe in et radice Melicerta. At urbes aer iacuit amor rebus, hic quem Arcesius? Crescit spatium sed tamen summis; monte erat oderat tantae heres lumine, cum tulit blandis; levibus ipse.</p> <ul> <li>Templa tu ossa sum at Nesso</li> <li>Senilibus nec</li> <li>Cornua ruunt Atlantiades simul prosunt</li> <li>Tela iube</li> <li>Demersit silvarum opto</li> <li>Dixerat mihi sunt relictis tyrannidis figura</li> </ul> <p>Vixque accipiter vestigia cum in hactenus viribus communis vasto? Mala quem alvum ad vetetve coeunt Psamathen [flavam] plumbum. Quotiens bellum intus medullis propioraque risisse, est inter pascere adpellare senectae palmis, legi Iuno aut [spernenda dedit Maeandrius]? Ante potius interdum deserti.</p>","tags":["LoremIpsum"]},{"location":"Miscellaneous/Templates/template_1/#andraemon-vidit","title":"Andraemon vidit","text":"<p>Turba est, et subito, esse, nobilitate terrae fassurae, tenebrisque. Petentia quo dixerat? Cetera apium mortalia, hic et exitiabile ego velate corpora [lyram Peneia] sparsa foedantem.</p> <ol> <li>Crotonis odio</li> <li>Python cadit negat venerisque delere rubescunt</li> <li>Marmore natum rettulit</li> <li>Moderantur cursu pello Apolline iactata</li> <li>Agmen longe Lelex quodque</li> <li>Coronatae carmen</li> </ol>","tags":["LoremIpsum"]},{"location":"Miscellaneous/Templates/template_1/#non-cedentique-nominat-rupit-gnato","title":"Non cedentique nominat rupit gnato","text":"<p>Auro gratum artus Achilles saniemque, fertque sub si haec. Facies mihi gaudetque induitur Pentheus erit inritaturque cum corpus cognorat lacertis sonantibus teli videtur ferat moderatus. Labores Thisbes, Troiana hoc Ortygiam fusus; adultera in et qui obortae partes etiamnum clarum letoque pavit puppe utque? Materque firmat fruges subito?</p> <p>In Ardea vulgaris [lapsae et acuti]. Iam natos letifera laevae. Pro haec inrita refugit illa. Nate abit et sola et silvas undas Egeriae corpora in cogeret? Flebile emisitque et inde.</p> <p>Quod rege coronas sine nisi torrem, adspicerent [dedit] haustos. Currum face matri letalem quoque et miserrima quae dicta ex utinam, nitidum. Illa medii dextra placido quoque structoque moratus plebe, lux in alii est, ubi.</p> <p>Quinque mundi Iphis nostras Peleu conscelero iunxit. Est Caras annos obicit, perlucentes iugis, subiectis sui. Humus et potiora placetque foribusque, promissaque serpens dispensat ventris abstulerunt. Novissima evolvit spes, convertit vellera Clytiumque intortos spem subcrescit etiam at dea pariter fortuna iussit, nullos, causam. Novo summa verba ponto, ille ignes faciem, eris qua; et est vestibus sibila mille lacertis saxo vulnere.</p>","tags":["LoremIpsum"]},{"location":"Teaching/stuff_1/","title":"Stuff 1","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_1/explanation_1/","title":"Overview Topic 1","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_1/Subtopic_1-1/explanation_1-1p1/","title":"Explanation 1","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_1/Subtopic_1-1/explanation_1-1p2/","title":"Explanation 2","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_1/Subtopic_1-2/explanation_1-2/","title":"Explanation 1","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_2/explanation_2/","title":"Explanation 2","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_3/explanation_3/","title":"Explanation 3","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_4/explanation_4/","title":"Explanation 4","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Theoretical_backgrounds/Topic_5/explanation_5/","title":"Explanation 5","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Work_at_MHH/AG_Depledge/rules/","title":"General Information","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Work_at_MHH/Institute_of_Virology/ags/","title":"Work Groups (AGs)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Work_at_MHH/Institute_of_Virology/general_events/","title":"Regular Events","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Work_at_MHH/Institute_of_Virology/rules_and_info/","title":"General Information","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Work_at_MHH/MHH_processes/poster_printing/","title":"How to print posters","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"},{"location":"Work_at_MHH/MHH_processes/sharepoint_and_documents/","title":"Where to find documents (SharePoint)","text":"<p>Work in progress</p> <p>This site is currently under construction, many parts still have an unfinished look or are yet to be written. For a full view of currently available pages check out the tag directory. ~Erik</p>"}]}